{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Domain Adaptation in Computer Vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook aims to prove if Domain Adaptation, specifically CORAL Loss, is effective at improving CNN accuracy.\n",
    "\n",
    "Paper found here: https://drive.google.com/file/d/16Gz5mhpFM9PfxREZWrYSjC4fnOsGTwbf/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " This project uses the Office-31 dataset, shortened down from 35 classes to 7 classes to reduce operations\n",
    "\n",
    "## Office-31 Dataset:\n",
    " This dataset is comprised of 35 classes of objects and images taken through 3 different cameras: Webcam, DSLR, Amazon. These cameras vary in focus, haze, saturation, creating randomized errors that reflect real world Convolutional Neural Networks (CNN) applications.\n",
    "\n",
    " This project uses DSLR for training and Webcam for validation, testing the ability of the ResNet18 model to detect generalized, domain-invariant features which exist among all samples, through being exposed to an object, seen through the lens of a faulty camera, to a better quality camera.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Train Control ResNet-18\n",
    "\n",
    "This code segment will train a control ResNet-18 Model that uses weights from ImageNet weights. The model is trained on the DSLR domain, where objects are captured through the lens on DSLR, and then tested on Webcam domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 127 images belonging to 10 classes.\n",
      "Found 27 images belonging to 10 classes.\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
      "  self._warn_if_super_not_called()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 5s/step - accuracy: 0.2255 - loss: 2.5302 - val_accuracy: 0.8519 - val_loss: 1.2959\n",
      "Epoch 2/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 2s/step - accuracy: 0.9162 - loss: 0.3991 - val_accuracy: 0.8889 - val_loss: 0.8839\n",
      "Epoch 3/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.9737 - loss: 0.0631 - val_accuracy: 0.9259 - val_loss: 0.3255\n",
      "Epoch 4/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0095 - val_accuracy: 0.9630 - val_loss: 0.0923\n",
      "Epoch 5/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.9948 - loss: 0.0183 - val_accuracy: 1.0000 - val_loss: 0.0331\n",
      "Epoch 6/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 0.9895 - loss: 0.0171 - val_accuracy: 1.0000 - val_loss: 0.0152\n",
      "Epoch 7/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 0.0022 - val_accuracy: 1.0000 - val_loss: 0.0125\n",
      "Epoch 8/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 1.4144e-04 - val_accuracy: 1.0000 - val_loss: 0.0133\n",
      "Epoch 9/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 3.5107e-05 - val_accuracy: 1.0000 - val_loss: 0.0158\n",
      "Epoch 10/10\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 2.1697e-05 - val_accuracy: 1.0000 - val_loss: 0.0198\n",
      "Found 171 images belonging to 10 classes.\n",
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2s/step - accuracy: 1.0000 - loss: 5.3663e-04\n",
      "Webcam domain accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Load pre-trained ResNet18 model (using tf.keras.applications)\n",
    "def create_resnet18_model(input_shape=(224, 224, 3), num_classes=10):\n",
    "    # Use the ResNet50V2 implementation to simulate ResNet18\n",
    "    base_model = tf.keras.applications.ResNet50V2(\n",
    "        include_top=False, weights='imagenet', input_shape=input_shape)\n",
    "    \n",
    "    # Adding the custom classification head\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dense(1024, activation='relu')(x)\n",
    "    predictions = Dense(num_classes, activation='softmax')(x)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "    \n",
    "    # Freeze the layers of ResNet50V2 (simulating ResNet18) for transfer learning\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Data generators\n",
    "train_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "\n",
    "# Load the first 10 classes from the source domain (DSLR)\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    'Office-31/dslr',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=['back_pack','bike','bike_helmet','bookcase','bottle','calculator','desk_chair','desk_lamp','desktop_computer','file_cabinet']  # Assuming classes are labeled as '0' to '9'\n",
    ")\n",
    "\n",
    "val_generator = train_datagen.flow_from_directory(\n",
    "    'Office-31/dslr',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=['back_pack','bike','bike_helmet','bookcase','bottle','calculator','desk_chair','desk_lamp','desktop_computer','file_cabinet']\n",
    ")\n",
    "\n",
    "# Instantiate and train the ResNet18 model\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    model = create_resnet18_model(num_classes=10)\n",
    "    history = model.fit(train_generator, validation_data=val_generator, epochs=10)\n",
    "\n",
    "# Evaluate on Webcam domain (first 10 classes)\n",
    "webcam_datagen = ImageDataGenerator(rescale=1./255)\n",
    "webcam_generator = webcam_datagen.flow_from_directory(\n",
    "    'Office-31/webcam',\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    classes=['back_pack','bike','bike_helmet','bookcase','bottle','calculator','desk_chair','desk_lamp','desktop_computer','file_cabinet']\n",
    ")\n",
    "with tf.device(\"/GPU:0\"):\n",
    "    evaluation = model.evaluate(webcam_generator)\n",
    "    print(f\"Webcam domain accuracy: {evaluation[1] * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Data Analysis on Control ResNet-18\n",
    "We will extract precision, recall and F1 scores per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m3/3\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3s/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Get predictions\n",
    "y_pred = model.predict(webcam_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)  # For multi-class classification\n",
    "\n",
    "# Extract true labels from the generator\n",
    "y_true = webcam_generator.classes  # Assuming the generator contains true labels\n",
    "\n",
    "# Calculate precision, recall, and F1 score for each class\n",
    "precision_per_class = precision_score(y_true, y_pred_classes, average=None)\n",
    "recall_per_class = recall_score(y_true, y_pred_classes, average=None)\n",
    "f1_per_class = f1_score(y_true, y_pred_classes, average=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10344828, 0.04761905, 0.14285714, 0.16666667, 0.0625    ,\n",
       "       0.19354839, 0.22222222, 0.07692308, 0.04761905, 0.10526316])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10344828, 0.04761905, 0.14285714, 0.16666667, 0.0625    ,\n",
       "       0.19354839, 0.25      , 0.05555556, 0.04761905, 0.10526316])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall_per_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.10344828, 0.04761905, 0.14285714, 0.16666667, 0.0625    ,\n",
       "       0.19354839, 0.23529412, 0.06451613, 0.04761905, 0.10526316])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_per_class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Precision, Recall and F1 to local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df=pd.DataFrame({\n",
    "    \"precision\":precision_per_class,\n",
    "    \"recall\":recall_per_class,\n",
    "    \"f1\":f1_per_class,\n",
    "    \"categories\":['back_pack','bike','bike_helmet','bookcase','bottle','calculator','desk_chair','desk_lamp','desktop_computer','file_cabinet']\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>precision</th>\n",
       "      <th>recall</th>\n",
       "      <th>f1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>categories</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>back_pack</th>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.058824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bike</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bike_helmet</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bookcase</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bottle</th>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>calculator</th>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.176471</td>\n",
       "      <td>0.176471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desk_chair</th>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desk_lamp</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.055556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>desktop_computer</th>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>file_cabinet</th>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "      <td>0.052632</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  precision    recall        f1\n",
       "categories                                     \n",
       "back_pack          0.058824  0.058824  0.058824\n",
       "bike               0.117647  0.117647  0.117647\n",
       "bike_helmet        0.176471  0.176471  0.176471\n",
       "bookcase           0.166667  0.166667  0.166667\n",
       "bottle             0.062500  0.062500  0.062500\n",
       "calculator         0.176471  0.176471  0.176471\n",
       "desk_chair         0.117647  0.117647  0.117647\n",
       "desk_lamp          0.055556  0.055556  0.055556\n",
       "desktop_computer   0.142857  0.142857  0.142857\n",
       "file_cabinet       0.052632  0.052632  0.052632"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.set_index(\"categories\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAnYAAAHWCAYAAAD6oMSKAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAABdQklEQVR4nO3dd1gUV9sG8HsX2aWDioBYQFGDBRsgsTciGKMhYjcWVDQqNt5YSCzYgrF3TYwtUYPd+BrFIHbFhmLXKBYsFJUIigrCnu8PP+Z1BZTqwuT+XddcsmfOzDxnC95MW4UQQoCIiIiIij2lrgsgIiIiooLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEeUhb59+8Le3j5Xyxw8eBAKhQIHDx4slJrkSKFQIDAwUHq8Zs0aKBQK3LlzR2c15URe3h+Uc7/99hscHR2hr68PCwsLXZdTKBQKBfz8/D7Ktvh+/XdhsKMiIeM/9IzJwMAA1apVg5+fH+Li4nRdXrH07nNaokQJlCtXDn379sWDBw90Xd6/QosWLbReg7ena9euSf2mT5+ODh06wNraOlPYzYmLFy+iU6dOsLOzg4GBAcqVK4fPPvsMixYtKuARFb5r166hb9++cHBwwIoVK/Dzzz8X6vYCAwOzfY0UCgViY2MLdfv5kZSUhMmTJ6NOnTowMTGBoaEhatWqhbFjx+Lhw4e6Lo90pISuCyB625QpU1CpUiW8evUKR48exbJly7B7925cunQJRkZGH62OFStWQKPR5GqZZs2a4eXLl1CpVIVUVd68/ZyeOHECa9aswdGjR3Hp0iUYGBjoujzZK1++PIKCgjK129raSj+PHz8eNjY2qFevHvbu3Zur9R8/fhwtW7ZExYoV4evrCxsbG9y7dw8nTpzAggULMGzYsHyP4WM6ePAgNBoNFixYgCpVqny07S5btgwmJiaZ2ovqHsNbt27B3d0d0dHR6Ny5MwYOHAiVSoULFy5g5cqV2L59O/7++29dl0k6wGBHRUrbtm3h4uICABgwYABKly6NuXPn4o8//kD37t2zXCY5ORnGxsYFWoe+vn6ul1EqlUUyKL37nFpaWuLHH3/Ezp070aVLFx1XJ3/m5ub4+uuv39vn9u3bsLe3x+PHj1GmTJlcrX/69OkwNzfH6dOnM4WQ+Pj43JabLy9evMj3H2AZNRdkoMpJXZ06dYKlpWWBbbMwpaWloWPHjoiLi8PBgwfRpEkTrfnTp0/Hjz/+qKPqSNd4KJaKtFatWgF48x8f8OZcERMTE0RFReHzzz+HqakpevbsCQDQaDSYP38+atasCQMDA1hbW2PQoEH4559/Mq13z549aN68OUxNTWFmZgZXV1ds2LBBmp/VOSnBwcFwdnaWlnFycsKCBQuk+dmdY7d582Y4OzvD0NAQlpaW+PrrrzMdCs0Y14MHD+Dl5QUTExOUKVMG3377LdLT0/P8/GWladOmAICoqCit9mvXrqFTp04oVaoUDAwM4OLigp07d2Za/unTpxg1ahTs7e2hVqtRvnx59O7dG48fPwYApKamYuLEiXB2doa5uTmMjY3RtGlTHDhwoEDH8a6cbvfOnTtQKBSYPXs2fv75Zzg4OECtVsPV1RWnT5/OtN4dO3agVq1aMDAwQK1atbB9+/YCrz0/5z9FRUWhZs2aWQYhKyurTG3r1q1DgwYNYGRkhJIlS6JZs2b466+/tPosXboUNWvWhFqthq2tLYYOHYqnT59q9WnRogVq1aqFiIgINGvWDEZGRvjuu+8AACkpKZg0aRKqVKkCtVqNChUqYMyYMUhJSXnvWOzt7TFp0iQAQJkyZTIdls5vXfmRm/d1xh5HJycnGBgYoEyZMvD09MSZM2cy9c14f6nVatSsWRMhISEfrGXr1q04f/48vv/++0yhDgDMzMwwffr0965j9uzZaNSoEUqXLg1DQ0M4Oztjy5YtmfqFhoaiSZMmsLCwgImJCT755JNMz+eiRYtQs2ZN6T3l4uKi9fuUPi7usaMiLSN8lC5dWmpLS0uDh4cHmjRpgtmzZ0t/iQ8aNAhr1qyBj48Phg8fjtu3b2Px4sU4d+4cjh07Ju2FW7NmDfr164eaNWsiICAAFhYWOHfuHEJCQtCjR48s6wgNDUX37t3RunVr6S/hq1ev4tixYxgxYkS29WfU4+rqiqCgIMTFxWHBggU4duwYzp07p/WfcXp6Ojw8PODm5obZs2dj3759mDNnDhwcHDB48OB8PY9vy7gwoWTJklLb5cuX0bhxY5QrVw7jxo2DsbExNm3aBC8vL2zduhVfffUVAOD58+do2rQprl69in79+qF+/fp4/Pgxdu7cifv378PS0hJJSUn45Zdf0L17d/j6+uLZs2dYuXIlPDw8cOrUKdStW7fAxvK23G53w4YNePbsGQYNGgSFQoGZM2eiY8eOuHXrlvRe+euvv+Dt7Y0aNWogKCgIT548gY+PD8qXL5/jutLT06XQm8HAwCDLw355YWdnh/DwcFy6dAm1atV6b9/JkycjMDAQjRo1wpQpU6BSqXDy5Ens378fbdq0AfDmnLPJkyfD3d0dgwcPxvXr17Fs2TKcPn1a63MEAE+ePEHbtm3RrVs3fP3117C2toZGo0GHDh1w9OhRDBw4ENWrV8fFixcxb948/P3339ixY0e29c2fPx+//vortm/fLh0arV27doHU9SEJCQmZ2kqUKCF9RnPz/urfvz/WrFmDtm3bYsCAAUhLS8ORI0dw4sQJae85ABw9ehTbtm3DkCFDYGpqioULF8Lb2xvR0dFav/PelfEHV69evT44ruwsWLAAHTp0QM+ePZGamorg4GB07twZu3btQrt27QC8+b3wxRdfoHbt2pgyZQrUajVu3ryJY8eOSetZsWIFhg8fjk6dOmHEiBF49eoVLly4gJMnT2b7+5QKmSAqAlavXi0AiH379olHjx6Je/fuieDgYFG6dGlhaGgo7t+/L4QQok+fPgKAGDdunNbyR44cEQDE+vXrtdpDQkK02p8+fSpMTU2Fm5ubePnypVZfjUYj/dynTx9hZ2cnPR4xYoQwMzMTaWlp2Y7hwIEDAoA4cOCAEEKI1NRUYWVlJWrVqqW1rV27dgkAYuLEiVrbAyCmTJmitc569eoJZ2fnbLf5Plk9p1u2bBFlypQRarVa3Lt3T+rbunVr4eTkJF69eiW1aTQa0ahRI1G1alWpbeLEiQKA2LZtW6btZTx/aWlpIiUlRWveP//8I6ytrUW/fv202gGISZMmZar59u3buR5vTrd7+/ZtAUCULl1aJCQkSO1//PGHACD++9//Sm1169YVZcuWFU+fPpXa/vrrLwFA6/2RnebNmwsAmaY+ffpk2f/Ro0eZnpMP+euvv4Senp7Q09MTDRs2FGPGjBF79+4VqampWv1u3LghlEql+Oqrr0R6errWvIzXLj4+XqhUKtGmTRutPosXLxYAxKpVqzKNbfny5Vrr+u2334RSqRRHjhzRal++fLkAII4dO/be8UyaNEkAEI8ePZLaCqKuD20vq+mTTz6R+uX0/bV//34BQAwfPjzTtt7+HQNAqFQqcfPmTant/PnzAoBYtGjRe2uuV6+eMDc3z9H4hMj8+0wIIV68eKH1ODU1VdSqVUu0atVKaps3b16m1+JdX375pahZs2aOa6HCx0OxVKS4u7ujTJkyqFChArp16wYTExNs374d5cqV0+r37h6szZs3w9zcHJ999hkeP34sTc7OzjAxMZEOl4SGhuLZs2cYN25cpvPhFApFtnVZWFggOTkZoaGhOR7LmTNnEB8fjyFDhmhtq127dnB0dMSff/6ZaZlvvvlG63HTpk1x69atHG8zK28/p506dYKxsTF27twp7XVKSEjA/v370aVLFzx79kx67p48eQIPDw/cuHFDOnS8detW1KlTR9qD97aM509PT0+6gESj0SAhIQFpaWlwcXHB2bNn8zWW98ntdrt27aq11zLjEHXG8x0TE4PIyEj06dMH5ubmUr/PPvsMNWrUyHFd9vb2CA0N1ZrGjBmTpzFm5bPPPkN4eDg6dOiA8+fPY+bMmfDw8EC5cuW0DqXv2LEDGo0GEydOhFKp/as/47Xbt28fUlNTMXLkSK0+vr6+MDMzy/SeVavV8PHx0WrbvHkzqlevDkdHR63PYsZpFXk5JF8QdX3I1q1bM71Oq1evlubn9P21detWKBQK6ZDy2979HePu7g4HBwfpce3atWFmZvbBz3xSUhJMTU1zNb53GRoaSj//888/SExMRNOmTbXGkrG38o8//sj2YjILCwvcv38/y9MYSDd4KJaKlCVLlqBatWooUaIErK2t8cknn2T6T6hEiRKZDoXduHEDiYmJWZ5TBPzvhOyMQ7sfOmT1riFDhmDTpk1o27YtypUrhzZt2qBLly7w9PTMdpm7d+8CAD755JNM8xwdHXH06FGttoxzcd5WsmTJLM8RzI2M5zQxMRGrVq3C4cOHoVarpfk3b96EEAITJkzAhAkTslxHfHw8ypUrh6ioKHh7e39wm2vXrsWcOXNw7do1vH79WmqvVKlSvsZSkNutWLGi1uOMkJfxfGe8flWrVs207CeffJLjkGpsbAx3d/ecDSCPXF1dsW3bNqSmpuL8+fPYvn075s2bh06dOiEyMhI1atRAVFQUlErle0Npdu9ZlUqFypUrS/MzlCtXLtNV4Ddu3MDVq1ezvQgkLxd0FERdH9KsWbMPXjyRk/dXVFQUbG1tUapUqQ9u8933IJCzz3xOwt+H7Nq1C9OmTUNkZKTWuY9vh8+uXbvil19+wYABAzBu3Di0bt0aHTt2RKdOnaTfy2PHjsW+ffvQoEEDVKlSBW3atEGPHj3QuHHjfNVHecdgR0VKgwYNtM5ByYparc4U9jQaDaysrLB+/fosl8ntlYbvsrKyQmRkJPbu3Ys9e/Zgz549WL16NXr37o21a9fma90Z9PT0CmQ973r7OfXy8kKTJk3Qo0cPXL9+HSYmJtJf4t9++y08PDyyXEdubjuxbt069O3bF15eXhg9ejSsrKygp6eHoKCgTBdsFKTcbje751sIUWg1FjaVSgVXV1e4urqiWrVq8PHxwebNm7Pce1QQ3t7rk0Gj0cDJyQlz587NcpkKFSoUSi1vy6qu/CqM93Ve34OOjo44d+4c7t27l6fn88iRI+jQoQOaNWuGpUuXomzZstDX18fq1au1LnowNDTE4cOHceDAAfz5558ICQnBxo0b0apVK/z111/Q09ND9erVcf36dezatQshISHYunUrli5diokTJ2Ly5Mm5ro3yj8GOZMHBwQH79u1D48aN3/tLPeOwx6VLl3J9jyyVSoX27dujffv20Gg0GDJkCH766SdMmDAhy3XZ2dkBAK5fvy4dhspw/fp1af7HlPEfUcuWLbF48WKMGzcOlStXBvDmFi8f2rPk4OCAS5cuvbfPli1bULlyZWzbtk3rr//CCheFtd2M1+fGjRuZ5l2/fj1vRX5EGWE+JiYGwJvXTqPR4MqVK9lewPL2ezbjfQG8uSL09u3bOdrz6ODggPPnz6N169bvPb0hNwqirvzK6fvLwcEBe/fuRUJCQo722uVF+/bt8fvvv2PdunUICAjI9fJbt26FgYEB9u7dq7X3/u1DzxmUSiVat26N1q1bY+7cufjhhx/w/fff48CBA9LzbmxsjK5du6Jr165ITU1Fx44dMX36dAQEBBTJW0DJHc+xI1no0qUL0tPTMXXq1Ezz0tLSpFsitGnTBqampggKCsKrV6+0+r3vr+QnT55oPVYqldLVetndwsHFxQVWVlZYvny5Vp89e/bg6tWr0pVnH1uLFi3QoEEDzJ8/H69evYKVlRVatGiBn376SQoBb3v06JH0s7e3t3So710Zz1/GXoi3n8+TJ08iPDy8oIeipaC3W7ZsWdStWxdr165FYmKi1B4aGoorV67kr9gCdODAgSzfu7t37wbwv8OXXl5eUCqVmDJlSqbzpTKWd3d3h0qlwsKFC7XWuXLlSiQmJuboPdulSxc8ePAAK1asyDTv5cuXSE5Ozvng/l9B1JVfOX1/eXt7QwiR5d6qgtob3KlTJzg5OWH69OlZvr+fPXuG77//Ptvl9fT0oFAotG6ldOfOnUxXLGd1pXDGHwUZv9Pe/d2oUqlQo0YNCCG0DlfTx8M9diQLzZs3x6BBgxAUFITIyEi0adMG+vr6uHHjBjZv3owFCxagU6dOMDMzw7x58zBgwAC4urqiR48eKFmyJM6fP48XL15ke1h1wIABSEhIQKtWrVC+fHncvXsXixYtQt26dVG9evUsl9HX18ePP/4IHx8fNG/eHN27d5dud2Jvb49Ro0blaax9+/bF2rVrpZva5sXo0aPRuXNnrFmzBt988w2WLFmCJk2awMnJCb6+vqhcuTLi4uIQHh6O+/fv4/z589JyW7ZsQefOndGvXz84OzsjISEBO3fuxPLly1GnTh188cUX2LZtG7766iu0a9cOt2/fxvLly1GjRg08f/4817Vm3DJm9erV6Nu3b7b9Cnq7ABAUFIR27dqhSZMm6NevHxISEqR7duV1nVn57bffcPfuXbx48QIAcPjwYUybNg3Am1tavG/v7rBhw/DixQt89dVXcHR0RGpqKo4fP46NGzfC3t5euoigSpUq+P777zF16lQ0bdoUHTt2hFqtxunTp2Fra4ugoCCUKVMGAQEBmDx5Mjw9PdGhQwdcv34dS5cuhaur6wdvtJxR76ZNm/DNN9/gwIEDaNy4MdLT03Ht2jVs2rQJe/fu/eDpFu8qiLo+ZMuWLVneguazzz6DtbV1jt9fLVu2RK9evbBw4ULcuHEDnp6e0Gg0OHLkCFq2bFkg3w+rr6+Pbdu2wd3dHc2aNUOXLl3QuHFj6Ovr4/Lly9iwYQNKliyZ7b3s2rVrh7lz58LT0xM9evRAfHw8lixZgipVquDChQtSvylTpuDw4cNo164d7OzsEB8fj6VLl6J8+fLS/fPatGkDGxsbNG7cGNbW1rh69SoWL16Mdu3a5fsCD8ojHVyJS5RJxm0uTp8+/d5+ffr0EcbGxtnO//nnn4Wzs7MwNDQUpqamwsnJSYwZM0Y8fPhQq9/OnTtFo0aNhKGhoTAzMxMNGjQQv//+u9Z23r49wJYtW0SbNm2ElZWVUKlUomLFimLQoEEiJiZG6vPu7U4ybNy4UdSrV0+o1WpRqlQp0bNnT+n2LR8aV8atGN7m7e0tDA0NxT///JPt8yDE+5/T9PR04eDgIBwcHKRbuERFRYnevXsLGxsboa+vL8qVKye++OILsWXLFq1lnzx5Ivz8/ES5cuWESqUS5cuXF3369BGPHz8WQry5pcMPP/wg7OzshFqtFvXq1RO7du3K8pYLyMHtThYtWiQAiJCQkPeON6fbzbjdyaxZszKt4916hBBi69atonr16kKtVosaNWqIbdu2ZTmWrDRv3jxHt4LI7rYoWb2f3rVnzx7Rr18/4ejoKExMTIRKpRJVqlQRw4YNE3FxcZn6r1q1Sno/lixZUjRv3lyEhoZq9Vm8eLFwdHQU+vr6wtraWgwePDjT++19Y0tNTRU//vijqFmzprQdZ2dnMXnyZJGYmPje8WR1u5OCqut92/vQ85+b93VaWpqYNWuWcHR0FCqVSpQpU0a0bdtWRERESH0AiKFDh2aqx87OLtvb4bzrn3/+ERMnThROTk7CyMhIGBgYiFq1aomAgACt301Z1bhy5UpRtWpVoVarhaOjo1i9enWm3zdhYWHiyy+/FLa2tkKlUglbW1vRvXt38ffff0t9fvrpJ9GsWTNRunRpoVarhYODgxg9evQHX2cqPAohivGZwkT/QtbW1ujduzdmzZql61I+ii5duuDOnTs4deqUrkshIiryeCiWqBi5fPkyXr58ibFjx+q6lI9CCIGDBw9i3bp1ui6FiKhY4B47IiIiIpngVbFEREREMsFgR0RERCQTDHZEREREMsFgR0RERCQTvCo2jzQaDR4+fAhTU9MC+9ocIiIioncJIfDs2TPY2tpm+q70dzHY5dHDhw8/ypdZExEREQHAvXv3UL58+ff2YbDLo4yvSrl37x7MzMx0XA0RERHJVVJSEipUqJCjr2ljsMujjMOvZmZmDHZERERU6HJy6hcvniAiIiKSCQY7IiIiIplgsCMiIiKSCZ5jR0REJCPp6el4/fq1rsugXNDX14eenl6BrIvBjoiISAaEEIiNjcXTp091XQrlgYWFBWxsbPJ9b1wGOyIiIhnICHVWVlYwMjLizfOLCSEEXrx4gfj4eABA2bJl87U+BjsiIqJiLj09XQp1pUuX1nU5lEuGhoYAgPj4eFhZWeXrsCwvniAiIirmMs6pMzIy0nEllFcZr11+z49ksCMiIpIJHn4tvgrqtWOwIyIiIpIJBjsiIiL6Vzl48CAUCkWOriDOTd+igBdPEBERydjHPjorxMfdXl40atQIMTExMDc3L9C+RQH32BEREVGxkZqamu91qFSqHN8zLjd9iwIGOyIiItKZFi1awM/PD35+fjA3N4elpSUmTJgA8f+7/uzt7TF16lT07t0bZmZmGDhwIADg6NGjaNq0KQwNDVGhQgUMHz4cycnJ0npTUlIwduxYVKhQAWq1GlWqVMHKlSsBZD68evfuXbRv3x4lS5aEsbExatasid27d2fZFwC2bt2KmjVrQq1Ww97eHnPmzNEak729PX744Qf069cPpqamqFixIn7++efCegq16DzYLVmyBPb29jAwMICbmxtOnTqVbd/Lly/D29sb9vb2UCgUmD9/fqY+GfPenYYOHSr1adGiRab533zzTWEMj4iIiD5g7dq1KFGiBE6dOoUFCxZg7ty5+OWXX6T5s2fPRp06dXDu3DlMmDABUVFR8PT0hLe3Ny5cuICNGzfi6NGj8PPzk5bp3bs3fv/9dyxcuBBXr17FTz/9BBMTkyy3P3ToUKSkpODw4cO4ePEifvzxx2z7RkREoEuXLujWrRsuXryIwMBATJgwAWvWrNHqN2fOHLi4uODcuXMYMmQIBg8ejOvXr+f/yfoQoUPBwcFCpVKJVatWicuXLwtfX19hYWEh4uLisux/6tQp8e2334rff/9d2NjYiHnz5mXqEx8fL2JiYqQpNDRUABAHDhyQ+jRv3lz4+vpq9UtMTMxV7YmJiQJArpcjIiIqaC9fvhRXrlwRL1++zDTvzVlvH2/KrebNm4vq1asLjUYjtY0dO1ZUr15dCCGEnZ2d8PLy0lqmf//+YuDAgVptR44cEUqlUrx8+VJcv35dABChoaFZbvPAgQMCgPjnn3+EEEI4OTmJwMDAHPXt0aOH+Oyzz7T6jB49WtSoUUN6bGdnJ77++mvpsUajEVZWVmLZsmXZPg/vew1zkzl0evHE3Llz4evrCx8fHwDA8uXL8eeff2LVqlUYN25cpv6urq5wdXUFgCznA0CZMmW0Hs+YMQMODg5o3ry5VruRkRFsbGwKYhj0L5KnUywCc7+QCMztAsXgbGX6oFy/vz7Gewvg+4sK3aeffqp1DlvDhg0xZ84cpKenAwBcXFy0+p8/fx4XLlzA+vXrpTYhBDQaDW7fvo2LFy9CT08v0//92Rk+fDgGDx6Mv/76C+7u7vD29kbt2rWz7Hv16lV8+eWXWm2NGzfG/PnzkZ6eLn1rxNvLKxQK2NjYSF8bVph0dig2NTUVERERcHd3/18xSiXc3d0RHh5eYNtYt24d+vXrl+mkx/Xr18PS0hK1atVCQEAAXrx4USDbJCIiooJlbGys9fj58+cYNGgQIiMjpen8+fO4ceMGHBwcpK/oyqkBAwbg1q1b6NWrFy5evAgXFxcsWrQoXzXr6+trPVYoFNBoNPlaZ07obI/d48ePkZ6eDmtra612a2trXLt2rUC2sWPHDjx9+hR9+/bVau/Rowfs7Oxga2uLCxcuYOzYsbh+/Tq2bduW7bpSUlKQkpIiPU5KSiqQGomIiP7tTp48qfX4xIkTqFq1arbfmVq/fn1cuXIFVapUyXK+k5MTNBoNDh06pLUD6X0qVKiAb775Bt988w0CAgKwYsUKDBs2LFO/6tWr49ixY1ptx44dQ7Vq1fL1Ha8FRdb3sVu5ciXatm0LW1tbrfaMK2qANy9+2bJl0bp1a0RFRcHBwSHLdQUFBWHy5MmFWi8REdG/UXR0NPz9/TFo0CCcPXsWixYtynSl6dvGjh2LTz/9FH5+fhgwYACMjY1x5coVhIaGYvHixbC3t0efPn3Qr18/LFy4EHXq1MHdu3cRHx+PLl26ZFrfyJEj0bZtW1SrVg3//PMPDhw4gOrVq2e57f/85z9wdXXF1KlT0bVrV4SHh2Px4sVYunRpgT0f+aGzQ7GWlpbQ09NDXFycVntcXFyBnPt29+5d7Nu3DwMGDPhgXzc3NwDAzZs3s+0TEBCAxMREabp3716+ayQiIqI3V7C+fPkSDRo0wNChQzFixAitnTDvql27Ng4dOoS///4bTZs2Rb169TBx4kStHTnLli1Dp06dMGTIEDg6OsLX11frdihvS09Px9ChQ1G9enV4enqiWrVq2Qa1+vXrY9OmTQgODkatWrUwceJETJkyJdPRQV3R2R47lUoFZ2dnhIWFwcvLCwCg0WgQFhamdblyXq1evRpWVlZo167dB/tGRkYCAMqWLZttH7VaDbVane+6iIiIPqbicO2Lvr4+5s+fj2XLlmWad+fOnSyXcXV1xV9//ZXtOg0MDDB37lzMnTs307wWLVpI98kD8N7z6d7tCwDe3t7w9vbOdpmsas7IGoVNp4di/f390adPH7i4uKBBgwaYP38+kpOTpatke/fujXLlyiEoKAjAm4shrly5Iv384MEDREZGwsTEROs4u0ajwerVq9GnTx+UKKE9xKioKGzYsAGff/45SpcujQsXLmDUqFFo1qxZtlfAEBERERUHOg12Xbt2xaNHjzBx4kTExsaibt26CAkJkS6oiI6OhlL5v6PFDx8+RL169aTHs2fPxuzZs9G8eXMcPHhQat+3bx+io6PRr1+/TNtUqVTYt2+fFCIrVKgAb29vjB8/vvAGSkRERPQRKMS7+xcpR5KSkmBubo7ExESYmZnpuhz6SHgfOypMvI8d5dWrV69w+/ZtVKpUCQYGBrouh/Lgfa9hbjKHzr9SjIiIiIgKBoMdERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BEREdG/SmBgIOrWrSs97tu3r/QtWMWdTm9QTERERIVLMTkvN+DMOzGJ9z3UJe6xIyIioiIjNTVV1yUUawx2REREpDMtWrSAn58fRo4cCUtLS3h4eODSpUto27YtTExMYG1tjV69euHx48fSMhqNBjNnzkSVKlWgVqtRsWJFTJ8+XZo/duxYVKtWDUZGRqhcuTImTJiA169f62J4Hx2DHREREenU2rVroVKpcOzYMcyYMQOtWrVCvXr1cObMGYSEhCAuLg5dunSR+gcEBGDGjBmYMGECrly5gg0bNkjfMw8ApqamWLNmDa5cuYIFCxZgxYoVmDdvni6G9tHxHDuifyl+LykRFRVVq1bFzJkzAQDTpk1DvXr18MMPP0jzV61ahQoVKuDvv/9G2bJlsWDBAixevBh9+vQBADg4OKBJkyZS//Hjx0s/29vb49tvv0VwcDDGjBnzkUakOwx2REREpFPOzs7Sz+fPn8eBAwdgYmKSqV9UVBSePn2KlJQUtG7dOtv1bdy4EQsXLkRUVBSeP3+OtLQ0mJmZFUrtRQ2DHREREemUsbGx9PPz58/Rvn17/Pjjj5n6lS1bFrdu3XrvusLDw9GzZ09MnjwZHh4eMDc3R3BwMObMmVPgdRdFDHZERERUZNSvXx9bt26Fvb09SpTIHFOqVq0KQ0NDhIWFYcCAAZnmHz9+HHZ2dvj++++ltrt37xZqzUUJL54gIiKiImPo0KFISEhA9+7dcfr0aURFRWHv3r3w8fFBeno6DAwMMHbsWIwZMwa//voroqKicOLECaxcuRLAm+AXHR2N4OBgREVFYeHChdi+fbuOR/XxMNgRERFRkWFra4tjx44hPT0dbdq0gZOTE0aOHAkLCwsolW9iy4QJE/Cf//wHEydORPXq1dG1a1fEx8cDADp06IBRo0bBz88PdevWxfHjxzFhwgRdDumjUgjBS9DyIikpCebm5khMTPzXnJBJebiSFPg4V5Pm4WPMq2KLHr4mlFevXr3C7du3UalSJRgYGOi6HMqD972Guckc3GNHREREJBMMdkREREQywWBHREREJBMMdkREREQywWBHREREJBMMdkREREQywWBHREREJBMMdkREREQywWBHREREJBMMdkRERKQzQggMHDgQpUqVgkKhQGRkpK5LKtYY7IiIiORMofi4Uy6FhIRgzZo12LVrF2JiYpCUlIT27dvD1tYWCoUCO3bsKPjnRMYY7IiIiEhnoqKiULZsWTRq1Ag2NjZITk5GnTp1sGTJEl2XViyV0HUBRERE9O/Ut29frF27FgCgUChgZ2eHO3fuoG3btjqurPhisCMiIiKdWLBgARwcHPDzzz/j9OnT0NPT03VJxR6DHREREemEubk5TE1NoaenBxsbG12XIws8x46IiIhIJhjsiIiIiGSCwY6IiIhIJniOHRERERUZz58/x82bN6XHt2/fRmRkJEqVKoWKFSvqsLLigcGOiIiIiowzZ86gZcuW0mN/f38AQJ8+fbBmzRodVVV8MNgRERHJmRC6ruC9Ro4ciZEjR0qPW7RoAVHEay7KeI4dERERkUww2BERERHJhM6D3ZIlS2Bvbw8DAwO4ubnh1KlT2fa9fPkyvL29YW9vD4VCgfnz52fqExgYCIVCoTU5Ojpq9Xn16hWGDh2K0qVLw8TEBN7e3oiLiyvooRERERF9VDoNdhs3boS/vz8mTZqEs2fPok6dOvDw8EB8fHyW/V+8eIHKlStjxowZ771Ddc2aNRETEyNNR48e1Zo/atQo/Pe//8XmzZtx6NAhPHz4EB07dizQsRERERF9bDoNdnPnzoWvry98fHxQo0YNLF++HEZGRli1alWW/V1dXTFr1ix069YNarU62/WWKFECNjY20mRpaSnNS0xMxMqVKzF37ly0atUKzs7OWL16NY4fP44TJ04U+BiJiIiIPhadBbvU1FRERETA3d39f8UolXB3d0d4eHi+1n3jxg3Y2tqicuXK6NmzJ6Kjo6V5EREReP36tdZ2HR0dUbFixfduNyUlBUlJSVoTERFRUaLRaHRdAuVRQb12OrvdyePHj5Geng5ra2utdmtra1y7di3P63Vzc8OaNWvwySefICYmBpMnT0bTpk1x6dIlmJqaIjY2FiqVChYWFpm2Gxsbm+16g4KCMHny5DzXRUREVFhUKhWUSiUePnyIMmXKQKVSQaFQ6LosygEhBFJTU/Ho0SMolUqoVKp8rU9297Fr27at9HPt2rXh5uYGOzs7bNq0Cf3798/zegMCAqSbJAJAUlISKlSokK9aiYiICoJSqUSlSpUQExODhw8f6rocygMjIyNUrFgRSmX+DqbqLNhZWlpCT08v09WocXFx770wIrcsLCxQrVo16etJbGxskJqaiqdPn2rttfvQdtVq9XvP6yMiItIllUqFihUrIi0tDenp6bouh3JBT08PJUqUKJC9rDoLdiqVCs7OzggLC4OXlxeAN8eXw8LC4OfnV2Dbef78OaKiotCrVy8AgLOzM/T19REWFgZvb28AwPXr1xEdHY2GDRsW2HaJiIg+NoVCAX19fejr6+u6FNIRnR6K9ff3R58+feDi4oIGDRpg/vz5SE5Oho+PDwCgd+/eKFeuHIKCggC8ueDiypUr0s8PHjxAZGQkTExMUKVKFQDAt99+i/bt28POzg4PHz7EpEmToKenh+7duwMAzM3N0b9/f/j7+6NUqVIwMzPDsGHD0LBhQ3z66ac6eBaIiIiICoZOg13Xrl3x6NEjTJw4EbGxsahbty5CQkKkCyqio6O1jjU/fPgQ9erVkx7Pnj0bs2fPRvPmzXHw4EEAwP3799G9e3c8efIEZcqUQZMmTXDixAmUKVNGWm7evHlQKpXw9vZGSkoKPDw8sHTp0o8zaCIiIqJCohD8pt08SUpKgrm5ORITE2FmZqbrcugjydPpD4G5X0gE5naB3H+Mcz2WjzEOoMh/YXlh4mtCRFnJTebQ+VeKEREREVHBYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgmdfvMEERERFb6PcXN13vy6aOAeOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZ0HmwW7JkCezt7WFgYAA3NzecOnUq276XL1+Gt7c37O3toVAoMH/+/Ex9goKC4OrqClNTU1hZWcHLywvXr1/X6tOiRQsoFAqt6ZtvvinooRERERF9VDoNdhs3boS/vz8mTZqEs2fPok6dOvDw8EB8fHyW/V+8eIHKlStjxowZsLGxybLPoUOHMHToUJw4cQKhoaF4/fo12rRpg+TkZK1+vr6+iImJkaaZM2cW+PiIiIiIPqYSutz43Llz4evrCx8fHwDA8uXL8eeff2LVqlUYN25cpv6urq5wdXUFgCznA0BISIjW4zVr1sDKygoRERFo1qyZ1G5kZJRtOCQiIiIqjnS2xy41NRURERFwd3f/XzFKJdzd3REeHl5g20lMTAQAlCpVSqt9/fr1sLS0RK1atRAQEIAXL14U2DaJiIiIdEFne+weP36M9PR0WFtba7VbW1vj2rVrBbINjUaDkSNHonHjxqhVq5bU3qNHD9jZ2cHW1hYXLlzA2LFjcf36dWzbti3bdaWkpCAlJUV6nJSUVCA1EhERERUUnR6KLWxDhw7FpUuXcPToUa32gQMHSj87OTmhbNmyaN26NaKiouDg4JDluoKCgjB58uRCrZeIiIgoP3R2KNbS0hJ6enqIi4vTao+LiyuQc9/8/Pywa9cuHDhwAOXLl39vXzc3NwDAzZs3s+0TEBCAxMREabp3716+ayQiIiIqSDoLdiqVCs7OzggLC5PaNBoNwsLC0LBhwzyvVwgBPz8/bN++Hfv370elSpU+uExkZCQAoGzZstn2UavVMDMz05qIiIiIihKdHor19/dHnz594OLiggYNGmD+/PlITk6WrpLt3bs3ypUrh6CgIABvLri4cuWK9PODBw8QGRkJExMTVKlSBcCbw68bNmzAH3/8AVNTU8TGxgIAzM3NYWhoiKioKGzYsAGff/45SpcujQsXLmDUqFFo1qwZateurYNngYiIiKhg6DTYde3aFY8ePcLEiRMRGxuLunXrIiQkRLqgIjo6Gkrl/3YqPnz4EPXq1ZMez549G7Nnz0bz5s1x8OBBAMCyZcsAvLkJ8dtWr16Nvn37QqVSYd++fVKIrFChAry9vTF+/PjCHSwRERFRIdP5xRN+fn7w8/PLcl5GWMtgb28PIcR71/eh+RUqVMChQ4dyVSMRERFRcaDzrxQjIiIiooLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDKh82+eoH8HhSKXCwTmdgFABOZ6EeAD31RCRLmX6887kOvPPD/vRFnjHjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimWCwIyIiIpIJBjsiIiIimSiQYJeUlIQdO3bg6tWrBbE6IiIiIsqDPAW7Ll26YPHixQCAly9fwsXFBV26dEHt2rWxdevWAi2QiIiIiHImT8Hu8OHDaNq0KQBg+/btEELg6dOnWLhwIaZNm5ardS1ZsgT29vYwMDCAm5sbTp06lW3fy5cvw9vbG/b29lAoFJg/f36e1vnq1SsMHToUpUuXhomJCby9vREXF5eruomIiIiKmjwFu8TERJQqVQoAEBISAm9vbxgZGaFdu3a4ceNGjtezceNG+Pv7Y9KkSTh79izq1KkDDw8PxMfHZ9n/xYsXqFy5MmbMmAEbG5s8r3PUqFH473//i82bN+PQoUN4+PAhOnbsmItngIiIiKjoyVOwq1ChAsLDw5GcnIyQkBC0adMGAPDPP//AwMAgx+uZO3cufH194ePjgxo1amD58uUwMjLCqlWrsuzv6uqKWbNmoVu3blCr1XlaZ2JiIlauXIm5c+eiVatWcHZ2xurVq3H8+HGcOHEil88EERERUdGRp2A3cuRI9OzZE+XLl0fZsmXRokULAG8O0To5OeVoHampqYiIiIC7u/v/ilEq4e7ujvDw8LyUlaN1RkRE4PXr11p9HB0dUbFixfduNyUlBUlJSVoTERERUVGSp2A3ZMgQhIeHY9WqVTh27BiUyjerqVy5co7PsXv8+DHS09NhbW2t1W5tbY3Y2Ni8lJWjdcbGxkKlUsHCwiJX2w0KCoK5ubk0VahQIU81EhERERWWPN/uxMXFBe3atcODBw+QlpYGAGjXrh0aN25cYMUVJQEBAUhMTJSme/fu6bokIiIiIi15CnYvXrxA//79YWRkhJo1ayI6OhoAMGzYMMyYMSNH67C0tISenl6mq1Hj4uKyvTCiINZpY2OD1NRUPH36NFfbVavVMDMz05qIiIiIipI8BbuAgACcP38eBw8e1LpYwt3dHRs3bszROlQqFZydnREWFia1aTQahIWFoWHDhnkpK0frdHZ2hr6+vlaf69evIzo6Os/bJSIiIioKSuRloR07dmDjxo349NNPoVAopPaaNWsiKioqx+vx9/dHnz594OLiggYNGmD+/PlITk6Gj48PAKB3794oV64cgoKCALy5OOLKlSvSzw8ePEBkZCRMTExQpUqVHK3T3Nwc/fv3h7+/P0qVKgUzMzMMGzYMDRs2xKeffpqXp4OIiIioSMhTsHv06BGsrKwytScnJ2sFvQ/p2rUrHj16hIkTJyI2NhZ169ZFSEiIdPFDdHS0dGEGADx8+BD16tWTHs+ePRuzZ89G8+bNcfDgwRytEwDmzZsHpVIJb29vpKSkwMPDA0uXLs3t00BERERUpOQp2Lm4uODPP//EsGHDAEAKc7/88kuuD2f6+fnBz88vy3kZYS2Dvb09hBD5WicAGBgYYMmSJViyZEmuaiUiIiIqyvIU7H744Qe0bdsWV65cQVpaGhYsWIArV67g+PHjOHToUEHXSEREREQ5kKeLJ5o0aYLz588jLS0NTk5O+Ouvv2BlZYXw8HA4OzsXdI1ERERElAO53mP3+vVrDBo0CBMmTMCKFSsKoyYiIiIiyoNc77HT19fH1q1bC6MWIiIiIsqHPB2K9fLywo4dOwq4FCIiIiLKjzxdPFG1alVMmTIFx44dg7OzM4yNjbXmDx8+vECKIyIiIqKcy1OwW7lyJSwsLBAREYGIiAiteQqFgsGOiIiISAfyFOxu375d0HUQERERUT7l6Ry7twkhcnTTYCIiIiIqXHkOdr/++iucnJxgaGgIQ0ND1K5dG7/99ltB1kZEREREuZCnQ7Fz587FhAkT4Ofnh8aNGwMAjh49im+++QaPHz/GqFGjCrRIIiIiIvqwPAW7RYsWYdmyZejdu7fU1qFDB9SsWROBgYEMdkREREQ6kKdDsTExMWjUqFGm9kaNGiEmJibfRRERERFR7uUp2FWpUgWbNm3K1L5x40ZUrVo130URERERUe7l6VDs5MmT0bVrVxw+fFg6x+7YsWMICwvLMvARERERUeHL0x47b29vnDx5EpaWltixYwd27NgBS0tLnDp1Cl999VVB10hEREREOZCnPXYA4OzsjHXr1hVkLURERESUD3naY7d7927s3bs3U/vevXuxZ8+efBdFRERERLmXp2A3btw4pKenZ2oXQmDcuHH5LoqIiIiIci9Pwe7GjRuoUaNGpnZHR0fcvHkz30URERERUe7lKdiZm5vj1q1bmdpv3rwJY2PjfBdFRERERLmXp2D35ZdfYuTIkYiKipLabt68if/85z/o0KFDgRVHRERERDmXp2A3c+ZMGBsbw9HREZUqVUKlSpXg6OiI0qVLY/bs2QVdIxERERHlQJ5ud2Jubo7jx48jNDQU58+fh6GhIerUqYOmTZsWdH1ERERElEO52mMXHh6OXbt2AQAUCgXatGkDKysrzJ49G97e3hg4cCBSUlIKpVAiIiIier9cBbspU6bg8uXL0uOLFy/C19cXn332GcaNG4f//ve/CAoKKvAiiYiIiOjDchXsIiMj0bp1a+lxcHAwGjRogBUrVsDf3x8LFy7kd8USERER6Uiugt0///wDa2tr6fGhQ4fQtm1b6bGrqyvu3btXcNURERERUY7lKthZW1vj9u3bAIDU1FScPXsWn376qTT/2bNn0NfXL9gKiYiIiChHchXsPv/8c4wbNw5HjhxBQEAAjIyMtK6EvXDhAhwcHAq8SCIiIiL6sFzd7mTq1Kno2LEjmjdvDhMTE6xduxYqlUqav2rVKrRp06bAiyQiIiKiD8tVsLO0tMThw4eRmJgIExMT6Onpac3fvHkzTExMCrRAIiIiIsqZPN+gOCulSpXKVzFERERElHd5+koxIiIiIip6GOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmGOyIiIiIZILBjoiIiEgmikSwW7JkCezt7WFgYAA3NzecOnXqvf03b94MR0dHGBgYwMnJCbt379aar1AospxmzZol9bG3t880f8aMGYUyPiIiIqKPQefBbuPGjfD398ekSZNw9uxZ1KlTBx4eHoiPj8+y//Hjx9G9e3f0798f586dg5eXF7y8vHDp0iWpT0xMjNa0atUqKBQKeHt7a61rypQpWv2GDRtWqGMlIiIiKkw6D3Zz586Fr68vfHx8UKNGDSxfvhxGRkZYtWpVlv0XLFgAT09PjB49GtWrV8fUqVNRv359LF68WOpjY2OjNf3xxx9o2bIlKleurLUuU1NTrX7GxsaFOlYiIiKiwqTTYJeamoqIiAi4u7tLbUqlEu7u7ggPD89ymfDwcK3+AODh4ZFt/7i4OPz555/o379/pnkzZsxA6dKlUa9ePcyaNQtpaWn5GA0RERGRbuXpu2ILyuPHj5Geng5ra2utdmtra1y7di3LZWJjY7PsHxsbm2X/tWvXwtTUFB07dtRqHz58OOrXr49SpUrh+PHjCAgIQExMDObOnZvlelJSUpCSkiI9TkpK+uD4iIiIiD4mnQa7j2HVqlXo2bMnDAwMtNr9/f2ln2vXrg2VSoVBgwYhKCgIarU603qCgoIwefLkQq+XiIiIKK90eijW0tISenp6iIuL02qPi4uDjY1NlsvY2NjkuP+RI0dw/fp1DBgw4IO1uLm5IS0tDXfu3MlyfkBAABITE6Xp3r17H1wnERER0cek02CnUqng7OyMsLAwqU2j0SAsLAwNGzbMcpmGDRtq9QeA0NDQLPuvXLkSzs7OqFOnzgdriYyMhFKphJWVVZbz1Wo1zMzMtCYiIiKiokTnh2L9/f3Rp08fuLi4oEGDBpg/fz6Sk5Ph4+MDAOjduzfKlSuHoKAgAMCIESPQvHlzzJkzB+3atUNwcDDOnDmDn3/+WWu9SUlJ2Lx5M+bMmZNpm+Hh4Th58iRatmwJU1NThIeHY9SoUfj6669RsmTJwh80ERERUSHQebDr2rUrHj16hIkTJyI2NhZ169ZFSEiIdIFEdHQ0lMr/7Vhs1KgRNmzYgPHjx+O7775D1apVsWPHDtSqVUtrvcHBwRBCoHv37pm2qVarERwcjMDAQKSkpKBSpUoYNWqU1nl3RERERMWNQgghdF1EcZSUlARzc3MkJibysGwOKBS5XCAwtwsAIjDXiwC5fPvnehzAxxlLHj7GcnlN5EQur8nH+JzwvZU7fE2Kt9xkDp3foJiIiIiICgaDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMMNgRERERyQSDHREREZFMlNB1AURE+aFQ5GGhwNwtJALzsA0h8rAQEVH+cI8dERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBG93UoR9jNs4AHm4lQNv40BERFQkcY8dERERkUww2BERERHJBIMdERERkUww2BERERHJBIMdERERkUww2BERERHJBG93QkRElI1c33aKt5wiHeMeOyIiIiKZYLAjIiIikgkGOyIiIiKZYLAjIiIikgkGOyIiIiKZ4FWxREREVGzwSuX3KxJ77JYsWQJ7e3sYGBjAzc0Np06dem//zZs3w9HREQYGBnBycsLu3bu15vft2xcKhUJr8vT01OqTkJCAnj17wszMDBYWFujfvz+eP39e4GMjIiIi+lh0Huw2btwIf39/TJo0CWfPnkWdOnXg4eGB+Pj4LPsfP34c3bt3R//+/XHu3Dl4eXnBy8sLly5d0urn6emJmJgYafr999+15vfs2ROXL19GaGgodu3ahcOHD2PgwIGFNk4iIiKiwqbzYDd37lz4+vrCx8cHNWrUwPLly2FkZIRVq1Zl2X/BggXw9PTE6NGjUb16dUydOhX169fH4sWLtfqp1WrY2NhIU8mSJaV5V69eRUhICH755Re4ubmhSZMmWLRoEYKDg/Hw4cNCHS8RERFRYdFpsEtNTUVERATc3d2lNqVSCXd3d4SHh2e5THh4uFZ/APDw8MjU/+DBg7CyssInn3yCwYMH48mTJ1rrsLCwgIuLi9Tm7u4OpVKJkydPZrndlJQUJCUlaU1ERERERYlOg93jx4+Rnp4Oa2trrXZra2vExsZmuUxsbOwH+3t6euLXX39FWFgYfvzxRxw6dAht27ZFenq6tA4rKyutdZQoUQKlSpXKdrtBQUEwNzeXpgoVKuR6vERERESFSZZXxXbr1k362cnJCbVr14aDgwMOHjyI1q1b52mdAQEB8Pf3lx4nJSUx3BEREVGRotM9dpaWltDT00NcXJxWe1xcHGxsbLJcxsbGJlf9AaBy5cqwtLTEzZs3pXW8e3FGWloaEhISsl2PWq2GmZmZ1kRERERUlOg02KlUKjg7OyMsLExq02g0CAsLQ8OGDbNcpmHDhlr9ASA0NDTb/gBw//59PHnyBGXLlpXW8fTpU0REREh99u/fD41GAzc3t/wMiYiIiEhndH5VrL+/P1asWIG1a9fi6tWrGDx4MJKTk+Hj4wMA6N27NwICAqT+I0aMQEhICObMmYNr164hMDAQZ86cgZ+fHwDg+fPnGD16NE6cOIE7d+4gLCwMX375JapUqQIPDw8AQPXq1eHp6QlfX1+cOnUKx44dg5+fH7p16wZbW9uP/yQQERERFQCdn2PXtWtXPHr0CBMnTkRsbCzq1q2LkJAQ6QKJ6OhoKJX/y5+NGjXChg0bMH78eHz33XeoWrUqduzYgVq1agEA9PT0cOHCBaxduxZPnz6Fra0t2rRpg6lTp0KtVkvrWb9+Pfz8/NC6dWsolUp4e3tj4cKFH3fwRERERAVI58EOAPz8/KQ9bu86ePBgprbOnTujc+fOWfY3NDTE3r17P7jNUqVKYcOGDbmqk4iIiKgo0/mhWCIiIiIqGAx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDJRJILdkiVLYG9vDwMDA7i5ueHUqVPv7b9582Y4OjrCwMAATk5O2L17tzTv9evXGDt2LJycnGBsbAxbW1v07t0bDx8+1FqHvb09FAqF1jRjxoxCGR8RERHRx6DzYLdx40b4+/tj0qRJOHv2LOrUqQMPDw/Ex8dn2f/48ePo3r07+vfvj3PnzsHLywteXl64dOkSAODFixc4e/YsJkyYgLNnz2Lbtm24fv06OnTokGldU6ZMQUxMjDQNGzasUMdKREREVJh0Huzmzp0LX19f+Pj4oEaNGli+fDmMjIywatWqLPsvWLAAnp6eGD16NKpXr46pU6eifv36WLx4MQDA3NwcoaGh6NKlCz755BN8+umnWLx4MSIiIhAdHa21LlNTU9jY2EiTsbFxoY+XiIiIqLDoNNilpqYiIiIC7u7uUptSqYS7uzvCw8OzXCY8PFyrPwB4eHhk2x8AEhMToVAoYGFhodU+Y8YMlC5dGvXq1cOsWbOQlpaW98EQERER6VgJXW788ePHSE9Ph7W1tVa7tbU1rl27luUysbGxWfaPjY3Nsv+rV68wduxYdO/eHWZmZlL78OHDUb9+fZQqVQrHjx9HQEAAYmJiMHfu3CzXk5KSgpSUFOlxUlJSjsZIRERE9LHoNNgVttevX6NLly4QQmDZsmVa8/z9/aWfa9euDZVKhUGDBiEoKAhqtTrTuoKCgjB58uRCr5mIiIgor3R6KNbS0hJ6enqIi4vTao+Li4ONjU2Wy9jY2OSof0aou3v3LkJDQ7X21mXFzc0NaWlpuHPnTpbzAwICkJiYKE337t37wOiIiIiIPi6dBjuVSgVnZ2eEhYVJbRqNBmFhYWjYsGGWyzRs2FCrPwCEhoZq9c8IdTdu3MC+fftQunTpD9YSGRkJpVIJKyurLOer1WqYmZlpTURERERFic4Pxfr7+6NPnz5wcXFBgwYNMH/+fCQnJ8PHxwcA0Lt3b5QrVw5BQUEAgBEjRqB58+aYM2cO2rVrh+DgYJw5cwY///wzgDehrlOnTjh79ix27dqF9PR06fy7UqVKQaVSITw8HCdPnkTLli1hamqK8PBwjBo1Cl9//TVKliypmyeCiIiIKJ90Huy6du2KR48eYeLEiYiNjUXdunUREhIiXSARHR0NpfJ/OxYbNWqEDRs2YPz48fjuu+9QtWpV7NixA7Vq1QIAPHjwADt37gQA1K1bV2tbBw4cQIsWLaBWqxEcHIzAwECkpKSgUqVKGDVqlNZ5d0RERETFjc6DHQD4+fnBz88vy3kHDx7M1Na5c2d07tw5y/729vYQQrx3e/Xr18eJEydyXScRERFRUabzGxQTERERUcFgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIplgsCMiIiKSCQY7IiIiIpkoEsFuyZIlsLe3h4GBAdzc3HDq1Kn39t+8eTMcHR1hYGAAJycn7N69W2u+EAITJ05E2bJlYWhoCHd3d9y4cUOrT0JCAnr27AkzMzNYWFigf//+eP78eYGPjYiIiOhj0Xmw27hxI/z9/TFp0iScPXsWderUgYeHB+Lj47Psf/z4cXTv3h39+/fHuXPn4OXlBS8vL1y6dEnqM3PmTCxcuBDLly/HyZMnYWxsDA8PD7x69Urq07NnT1y+fBmhoaHYtWsXDh8+jIEDBxb6eImIiIgKi86D3dy5c+Hr6wsfHx/UqFEDy5cvh5GREVatWpVl/wULFsDT0xOjR49G9erVMXXqVNSvXx+LFy8G8GZv3fz58zF+/Hh8+eWXqF27Nn799Vc8fPgQO3bsAABcvXoVISEh+OWXX+Dm5oYmTZpg0aJFCA4OxsOHDz/W0ImIiIgKlE6DXWpqKiIiIuDu7i61KZVKuLu7Izw8PMtlwsPDtfoDgIeHh9T/9u3biI2N1epjbm4ONzc3qU94eDgsLCzg4uIi9XF3d4dSqcTJkycLbHxEREREH1MJXW788ePHSE9Ph7W1tVa7tbU1rl27luUysbGxWfaPjY2V5me0va+PlZWV1vwSJUqgVKlSUp93paSkICUlRXqcmJgIAEhKSnrvGD+6Vx/u8q5cj+BjjPljjAOQz1jkMg6gSI5FLuMA5DMWuYwDKKKfd4CvSa4WKNxxZGQNIcQH++o02BUnQUFBmDx5cqb2ChUq6KCa95iR+0XMc71ArpfIvY8xDkA+Y5HLOIAiORa5jAOQz1jkMg6giH7eAb4muVrg47wmz549g/kHtqXTYGdpaQk9PT3ExcVptcfFxcHGxibLZWxsbN7bP+PfuLg4lC1bVqtP3bp1pT7vXpyRlpaGhISEbLcbEBAAf39/6bFGo0FCQgJKly4NhUKRg9HqTlJSEipUqIB79+7BzMxM1+Xki1zGIpdxAPIZi1zGAchnLHIZByCfsXAcuiGEwLNnz2Bra/vBvjoNdiqVCs7OzggLC4OXlxeAN4EpLCwMfn5+WS7TsGFDhIWFYeTIkVJbaGgoGjZsCACoVKkSbGxsEBYWJgW5pKQknDx5EoMHD5bW8fTpU0RERMDZ2RkAsH//fmg0Gri5uWW5XbVaDbVardVmYWGRx5HrhpmZWbF4A+eEXMYil3EA8hmLXMYByGcschkHIJ+xcBwf34f21GXQ+aFYf39/9OnTBy4uLmjQoAHmz5+P5ORk+Pj4AAB69+6NcuXKISgoCAAwYsQING/eHHPmzEG7du0QHByMM2fO4OeffwYAKBQKjBw5EtOmTUPVqlVRqVIlTJgwAba2tlJ4rF69Ojw9PeHr64vly5fj9evX8PPzQ7du3XKUhomIiIiKIp0Hu65du+LRo0eYOHEiYmNjUbduXYSEhEgXP0RHR0Op/N/Fu40aNcKGDRswfvx4fPfdd6hatSp27NiBWrVqSX3GjBmD5ORkDBw4EE+fPkWTJk0QEhICAwMDqc/69evh5+eH1q1bQ6lUwtvbGwsXLvx4AyciIiIqYDoPdgDg5+eX7aHXgwcPZmrr3LkzOnfunO36FAoFpkyZgilTpmTbp1SpUtiwYUOuay2O1Go1Jk2alOlQcnEkl7HIZRyAfMYil3EA8hmLXMYByGcsHEfRpxA5uXaWiIiIiIo8nX/zBBEREREVDAY7IiIiIplgsCMiIiKSCQY7IiIiIplgsJMxjUaD9PR0XZdBWeA1S0VHTEwMrly5ousy8i3jsy6H99aLFy+Qmpqq6zLy7f79+zh37pyuy6C3aDQaaDQaXZdRqBjsZOrKlSvo3bs3PDw8MHjwYBw/flzXJeWZXMJpcnIynj17hqSkpCL/NXTvk5CQgGvXruHGjRvF/j/fBw8ewMnJCePHj8eZM2d0XU6eRUZGwsvLCy9evCjW7y0AuHTpErp06YITJ04gJSVF1+Xk2eXLl9GoUSOsW7cOAIp1mLh//z42bdqEbdu24eLFi7ouJ8+uXLmCvn37wt3dHQMHDkRwcLCuSyoUDHYydP36dTRq1Ajp6elwdXVFeHg4RowYUSxvwPz3339j/vz5iImJ0XUp+XLlyhV07NgRzZs3R/Xq1bF+/XoAxW/vyqVLl+Du7o4uXbrAyckJM2fOLNbB+8aNG0hMTERiYiIWLVqEs2fPSvOKy2tz/vx5NGrUCDVr1oSRkZHUXlzqf9vly5fRtGlTlC9fHpUqVSq29xg7f/48GjRogBIlSmDDhg2Ij4/XutF+cXLx4kU0adIEs2bNwpAhQ/D9998jKipK12Xl2rVr19CkSROoVCp88cUXiI6OxoQJEzBs2DBdl1bwBMmKRqMR3333nejSpYvUlpSUJKZNmybq1q0rfvzxRx1Wlzs3btwQpUqVEgqFQgQEBIhHjx7puqQ8uXz5sihdurQYNWqUWL9+vfD39xf6+vri3Llzui4tVzLG8e2334rLly+L2bNnC4VCIaKjo3VdWp49efJEdOjQQfz000+ifv36omfPnuLSpUtCCCHS09N1XN2HnT9/XhgbG4vRo0drtaekpOioorx7/vy5aNOmjRg8eLDUdvXqVXHu3Dlx9+5dHVaWO5GRkcLQ0FB899134tGjR6JmzZpi2rRpQqPRCI1Go+vycuXOnTuiXLlyYty4ceL58+di9+7dwsbGRpw8eVLXpeXKq1evRM+ePcXw4cOltpcvX4p69eoJhUIhunfvrsPqCh6DnQz17dtXNGvWTKstKSlJzJ49W7i4uIh169bpqLKce/78uejXr5/o27evWLJkiVAoFGL06NHFLtw9efJEtGnTRusXihBCtGjRQgwbNkwIIYrFL/tHjx6JZs2aiREjRkhtGo1GeHp6iuPHj4tz584Vu4CXlpYm4uPjRbVq1cT9+/fFtm3bhKurq/D19RWNGjUS3t7eui7xvWJiYoSNjY3w8PAQQrwZz8iRI0W7du2Eo6OjmDdvnrh69aqOq8y5V69eiSZNmoizZ8+KtLQ04eHhIVxdXYWpqan49NNPxS+//KLrEj/o/PnzQq1Wi++++04I8eaPg06dOglXV1epT3H4vGf46aefRIsWLbRq/vzzz8VPP/0k1q5dK/bv36/D6nKndevWIjAwUAjxJtQJIcSYMWOEt7e3qF+/vpg1a5YuyytQxXPfMGVJ/P+hl/r16yM9PR3Xr1+X5pmamqJfv36oV68eli5dihcvXuiqzBxRKpVwdnaGp6cnhgwZguDgYMyePRszZ87E48ePdV1ejr1+/RpPnz5Fp06dAPzvPJtKlSohISEBAIrFOVEKhQKenp4YOnSo1DZt2jTs3bsXQ4YMQfv27eHr64ujR4/qsMrcUSqVKFOmDFxdXXHp0iV89dVXCAwMxPbt23Hx4kV88cUXui7xgxo2bIgnT57gjz/+wBdffIGLFy/C0dERrVu3xsKFCzF79mxER0fruswcefr0Ka5fv47Hjx9j9OjRAIBffvkFmzZtQtOmTTF+/Hhs2bJFx1W+X0pKCsaMGYPp06dDo9FAqVRi2rRp+Pvvv7Fs2TIAxePznkEIgejoaERGRgIApk+fjj179mDz5s1YvHgxunXrhjVr1ui0xg8RQkgX40RFRSEtLQ0GBgZ48OABNm7ciHbt2qFGjRrYvXu3rkstODoOllQIbt68KSwtLUW/fv3Es2fPhBD/+ysxOjpaKBQKsWfPHl2WmCPPnz/XehwcHCwUCoX49ttvxePHj4UQb/4ivnXrli7Ky7G///5b+jk1NVUIIcT48eNFr169tPplvFZFVVJSkvTz77//LhQKhdi4caN48uSJOHTokHB1dZX+Ii5OevfuLcaNGyeEEKJ///6iZMmSokaNGqJfv35F/pDTw4cPRe/evYWhoaH47LPPpM+FEEKsX79eWFhYiN27d+uwwpzTaDSiW7duws/PT3zxxRciJCREmnfv3j3x9ddfi2+++UakpaUVm71eGo1GPH36VHh5eYkuXboUq9qFEOLWrVuiUaNGokqVKsLb21soFAqxY8cOodFoRFxcnBg+fLho0aKFePz4cZEf19GjR4VSqRTNmjUTvXr1EsbGxmLAgAFCCCEuXrwoTE1NxbVr14r8OHKihK6DJRU8BwcHbNq0CW3btoWhoSECAwNhaWkJANDX10ft2rVhbm6u4yo/zNjYGMCbq2KVSiW6du0KIQR69OgBhUKBkSNHYvbs2bh79y5+++03rRPHi5KqVasCeLO3Tl9fH8CbvyLj4+OlPkFBQVCr1Rg+fDhKlCiaH0tTU1Pp54YNG+LMmTOoX78+AKBZs2awsrJCRESErsrLNSEEFAoFWrVqhdu3b2PIkCHYvXs3IiIiEBkZidGjR0OlUqF27dowMDDQdblZKlu2LIKCglCuXDm4u7ujdOnS0rh69OiBSZMm4cCBA2jbtq2uS/0ghUKB//znP2jRogVevHiBgQMHSvPKly8Pa2trnD59Gkqlstjs9VIoFDA3N0evXr3QqVMnDB8+HI0bN9Z1WTlWqVIlrFu3DqdPn8aVK1egUCjw5ZdfAgCsrKxga2uLQ4cOwdjYuMi/Jo0bN8aJEyewcOFCqNVqzJw5E0OGDAEA3Lp1C+XLl4eNjU2RH0dOFM3/QSjfWrZsic2bN6Nz586IiYlBly5dULt2bfz666+Ij49HhQoVdF1ijunp6UEIAY1Gg27dukGhUKBXr17YuXMnoqKicPr06SIb6t6mVCql/3QzHgPAxIkTMW3aNJw7d67Ihrp32dnZwc7ODsCbwJqamgoTExPUrl1bx5XlXMbrUKlSJfj4+MDa2hq7du1CpUqVUKlSJSgUCtSpU6fIhroMtra2GDdunFSnQqGAEAIJCQkoU6YM6tatq9sCc8HFxQV79uxB8+bN8fPPP6Ny5cqoWbMmgDenNVSrVg1paWnSH0jFxRdffIHPPvsMy5YtQ/369WFoaKjrknIs4/Pwyy+/4MyZM0hNTYVKpQIAxMXFwd7evthcGe/q6opff/01U3g7cuQIrK2tZRHqAPBQrNxFRESI5s2bCzs7O+Hg4CCqVasmzp49q+uy8uTtq8patWolSpUqJS5cuKDjqnIn40rLSZMmiYEDB4pZs2YJtVotIiIidFxZ/kyYMEFUrFhR67BzcZGamipWrlwpzp8/L4QoXie3v8/EiRNF1apVxZ07d3RdSq4dOnRI2NraigYNGoj+/fuLXr16CXNzc3Hx4kVdl5ZnQUFBwszMTMTExOi6lDy5fPmyMDc3FzNnzhS//vqrGDNmjLCwsCh2v4PfduHCBTFkyBBhZmYmIiMjdV1OgSkeuwcoz+rXr4+dO3ciISEBz549Q9myZaXDssWNQqFAeno6Ro8ejQMHDiAyMhJOTk66LitXMvbS6evrY8WKFTAzM8PRo0elQ5rFzebNm3Ho0CEEBwcjNDRUOuxcnOjr66Nv377Sa1Pc/2oPDg7GgQMHsHnzZoSFhUl7VouTZs2aYf/+/Vi3bh1OnDiBqlWr4ujRo6hVq5auS8s18f976QcNGoQtW7bg1atXui4pT2rUqIHt27fD19cXSqUS5cqVw6FDh4rd7+AMKSkpuHnzJhISEnDkyJFidbThQxRCFMO7WNK/Vnp6OtasWQNnZ+didYjpXWfOnEGDBg1w6dIl1KhRQ9fl5Nnly5cxZcoUBAYGonr16rouhwBcuHAB3333HX788UfpMGZxlnEleXG9wW8G8f9XZ2acO1xcJSQk4PXr11Cr1bCwsNB1OfmSkpKCtLS0Yv+avIvBjood8dZ5asVZcnKyLH6hvH79utid8yR3b58HRUT/Lgx2RERERDJRvPdtExEREZGEwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIiIiGSCwY6IiIhIJhjsiIhySaFQYMeOHboug4goEwY7IqJ3xMbGYtiwYahcuTLUajUqVKiA9u3bIywsTNelERG9F78rlojoLXfu3EHjxo1hYWGBWbNmwcnJCa9fv8bevXsxdOhQXLt2TdclEhFli3vsiIjeMmTIECgUCpw6dQre3t6oVq0aatasCX9/f5w4cSLLZcaOHYtq1arByMgIlStXxoQJE/D69Wtp/vnz59GyZUuYmprCzMwMzs7OOHPmDADg7t27aN++PUqWLAljY2PUrFkTu3fvlpa9dOkS2rZtCxMTE1hbW6NXr154/PixNH/Lli1wcnKCoaEhSpcuDXd3dyQnJxfSs0NERR332BER/b+EhASEhIRg+vTpWX6Pb3Zfem5qaoo1a9bA1tYWFy9ehK+vL0xNTTFmzBgAQM+ePVGvXj0sW7YMenp6iIyMlL5fd+jQoUhNTcXhw4dhbGyMK1euwMTEBADw9OlTtGrVCgMGDMC8efPw8uVLjB07Fl26dMH+/fsRExOD7t27Y+bMmfjqq6/w7NkzHDlyBPymSKJ/LwY7IqL/d/PmTQgh4OjomKvlxo8fL/1sb2+Pb7/9FsHBwVKwi46OxujRo6X1Vq1aVeofHR0Nb29vODk5AQAqV64szVu8eDHq1auHH374QWpbtWoVKlSogL///hvPnz9HWloaOnbsCDs7OwCQ1kNE/04MdkRE/y+ve7o2btyIhQsXIioqSgpbZmZm0nx/f38MGDAAv/32G9zd3dG5c2c4ODgAAIYPH47Bgwfjr7/+gru7O7y9vVG7dm0Abw7hHjhwQNqD97aoqCi0adMGrVu3hpOTEzw8PNCmTRt06tQJJUuWzNM4iKj44zl2RET/r2rVqlAoFLm6QCI8PBw9e/bE559/jl27duHcuXP4/vvvkZqaKvUJDAzE5cuX0a5dO+zfvx81atTA9u3bAQADBgzArVu30KtXL1y8eBEuLi5YtGgRAOD58+do3749IiMjtaYbN26gWbNm0NPTQ2hoKPbs2YMaNWpg0aJF+OSTT3D79u2CfWKIqNhQCJ6MQUQkadu2LS5evIjr169nOs/u6dOnsLCwgEKhwPbt2+Hl5YU5c+Zg6dKliIqKkvoNGDAAW7ZswdOnT7PcRvfu3ZGcnIydO3dmmhcQEIA///wTFy5cwPfff4+tW7fi0qVLKFHiwwdY0tPTYWdnB39/f/j7++du4EQkC9xjR0T0liVLliA9PR0NGjTA1q1bcePGDVy9ehULFy5Ew4YNM/WvWrUqoqOjERwcjKioKCxcuFDaGwcAL1++hJ+fHw4ePIi7d+/i2LFjOH36NKpXrw4AGDlyJPbu3Yvbt2/j7NmzOHDggDRv6NChSEhIQPfu3XH69GlERUVh79698PHxQXp6Ok6ePIkffvgBZ86cQXR0NLZt24ZHjx5JyxPRvw/PsSMiekvlypVx9uxZTJ8+Hf/5z38QExODMmXKwNnZGcuWLcvUv0OHDhg1ahT8/PyQkpKCdu3aYcKECQgMDAQA6Onp4cmTJ+jduzfi4uJgaWmJjh07YvLkyQDe7GUbOnQo7t+/DzMzM3h6emLevHkAAFtbWxw7dgxjx45FmzZtkJKSAjs7O3h6ekKpVMLMzAyHDx/G/PnzkZSUBDs7O8yZMwdt27b9aM8XERUtPBRLREREJBM8FEtEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLBYEdEREQkEwx2RERERDLxf8ZsSj3C7uH0AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Define the width of the bars\n",
    "bar_width = 0.25\n",
    "\n",
    "# Set the x locations for the groups\n",
    "x = np.arange(len(df.index))\n",
    "\n",
    "# Create bar plots\n",
    "plt.bar(x - bar_width, df['precision'], width=bar_width, label='precision', color='b')\n",
    "plt.bar(x, df['recall'], width=bar_width, label='recall', color='g')\n",
    "plt.bar(x + bar_width, df['f1'], width=bar_width, label='f1', color='r')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Classes')\n",
    "plt.ylabel('Scores')\n",
    "plt.title('Precision, Recall, and F1 Score for Each Class')\n",
    "plt.xticks(x, df.index, rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"Control - P,R,F.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define CORAL Loss\n",
    "\n",
    "CORAL (CORrelation ALignment) Loss is defined as the distance between the covariance matrices of the source and target feature embeddings. These embeddings are extracted from intermediate layers of the network. The objective is to minimize domain shift by reducing the discrepancy between the second-order statistics (i.e., covariances) of the source and target domains. In the context of the altered ResNet-18, CORAL loss encourages the network to learn features that are domain-invariant, hence aligning the distributions of the source and target embeddings on a multidimensional plane."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula is given for Coral:\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_{\\text{CORAL}} = \\frac{1}{4d^2} \\left\\| C_S - C_T \\right\\|_F^2\n",
    "$$\n",
    "\n",
    "where $ \\left\\|\\right\\|_F $ represent the Frobenius norm.\n",
    "\n",
    "The Frobenius norm, represented by:\n",
    "\n",
    "$$\n",
    "\\| A \\|_F = \\sqrt{ \\sum_{i=1}^{m} \\sum_{j=1}^{n} |a_{ij}|^2 }\n",
    "$$\n",
    "\n",
    "Calculates the distance between two covariance embeddings between the matrices\n",
    "\n",
    "In order to calculate second-order statistics (covariances):\n",
    "\n",
    "$$\n",
    "C = \\frac{1}{n - 1} X^\\top X - \\frac{1}{n(n - 1)} X^\\top \\mathbf{1} \\mathbf{1}^\\top X\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "X &\\in \\mathbb{R}^{n \\times d} \\quad &\\text{Feature matrix (}n\\text{ samples, }d\\text{ features)} \\\\\n",
    "\\mathbf{1} &\\in \\mathbb{R}^{n \\times 1} \\quad &\\text{Vector of all ones} \\\\\n",
    "X^\\top &\\in \\mathbb{R}^{d \\times n} \\quad &\\text{Transpose of }X \\\\\n",
    "C &\\in \\mathbb{R}^{d \\times d} \\quad &\\text{Covariance matrix} \\\\\n",
    "n &\\in \\mathbb{N} \\quad &\\text{Number of samples in the batch}\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading source training data...\n",
      "Found 127 images belonging to 10 classes.\n",
      "Loading target training data...\n",
      "Found 235 images belonging to 10 classes.\n",
      "Loading validation data...\n",
      "Found 27 images belonging to 10 classes.\n",
      "Creating ResNet18-like model from ResNet50...\n",
      "Freezing layers...\n",
      "Starting training...\n",
      "Iterations per epoch: 2\n",
      "\n",
      "Starting Epoch 1/10...\n",
      "Processing Batch 1/2...\n",
      "Source forward pass complete for Batch 1\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 186\u001b[0m\n\u001b[1;32m    184\u001b[0m coral_model \u001b[38;5;241m=\u001b[39m create_coral_resnet18_model(num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 186\u001b[0m total_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_coral_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[43m    \u001b[49m\u001b[43msource_train_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_train_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcoral_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource_val_gen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\n\u001b[1;32m    188\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 82\u001b[0m, in \u001b[0;36mtrain_coral_model\u001b[0;34m(source_data, target_data, model, val_data, epochs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSource forward pass complete for Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# Forward pass for target domain\u001b[39;00m\n\u001b[0;32m---> 82\u001b[0m target_features, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_images\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTarget forward pass complete for Batch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch_num\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m# Compute cross-entropy loss on source domain\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/layers/layer.py:882\u001b[0m, in \u001b[0;36mLayer.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    880\u001b[0m         outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    881\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 882\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m \u001b[38;5;66;03m# Change the layout for the layer output if needed.\u001b[39;00m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;66;03m# This is useful for relayout intermediate tensor in the model\u001b[39;00m\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# to achieve the optimal performance.\u001b[39;00m\n\u001b[1;32m    886\u001b[0m distribution \u001b[38;5;241m=\u001b[39m distribution_lib\u001b[38;5;241m.\u001b[39mdistribution()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:117\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/ops/operation.py:46\u001b[0m, in \u001b[0;36mOperation.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m             call_fn \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall\n\u001b[1;32m     42\u001b[0m     call_fn \u001b[38;5;241m=\u001b[39m traceback_utils\u001b[38;5;241m.\u001b[39minject_argument_info_in_traceback(\n\u001b[1;32m     43\u001b[0m         call_fn,\n\u001b[1;32m     44\u001b[0m         object_name\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.call()\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     45\u001b[0m     )\n\u001b[0;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Plain flow.\u001b[39;00m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m any_symbolic_tensors(args, kwargs):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:156\u001b[0m, in \u001b[0;36minject_argument_info_in_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    154\u001b[0m bound_signature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(e, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_keras_call_info_injected\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    159\u001b[0m         \u001b[38;5;66;03m# Only inject info for the innermost failing call\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/models/functional.py:167\u001b[0m, in \u001b[0;36mFunctional.call\u001b[0;34m(self, inputs, training, mask)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, training\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, mask\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;66;03m# Add support for traning, masking\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_standardize_inputs\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    169\u001b[0m         masks \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;01mNone\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(inputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/models/functional.py:258\u001b[0m, in \u001b[0;36mFunctional._standardize_inputs\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_standardize_inputs\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs):\n\u001b[1;32m    257\u001b[0m     flat_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flatten_to_reference_inputs(inputs)\n\u001b[0;32m--> 258\u001b[0m     flat_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_inputs_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_adjust_input_rank(flat_inputs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/models/functional.py:218\u001b[0m, in \u001b[0;36mFunctional._convert_inputs_to_tensors\u001b[0;34m(self, flat_inputs)\u001b[0m\n\u001b[1;32m    215\u001b[0m         converted\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m         converted\u001b[38;5;241m.\u001b[39mappend(\n\u001b[0;32m--> 218\u001b[0m             \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[43m                \u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\n\u001b[1;32m    220\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    221\u001b[0m         )\n\u001b[1;32m    222\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m converted\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/ops/core.py:743\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse)\u001b[0m\n\u001b[1;32m    724\u001b[0m \u001b[38;5;129m@keras_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkeras.ops.convert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor\u001b[39m(x, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    726\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Convert a NumPy array to a tensor.\u001b[39;00m\n\u001b[1;32m    727\u001b[0m \n\u001b[1;32m    728\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;124;03m    >>> y = keras.ops.convert_to_tensor(x)\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 743\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbackend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/keras/src/backend/tensorflow/core.py:113\u001b[0m, in \u001b[0;36mconvert_to_tensor\u001b[0;34m(x, dtype, sparse)\u001b[0m\n\u001b[1;32m    111\u001b[0m         x \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mconvert_to_tensor(x)\n\u001b[1;32m    112\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mcast(x, dtype)\n\u001b[0;32m--> 113\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m dtype:\n\u001b[1;32m    115\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, tf\u001b[38;5;241m.\u001b[39mSparseTensor):\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1260\u001b[0m, in \u001b[0;36madd_dispatch_support.<locals>.decorator.<locals>.op_dispatch_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1258\u001b[0m \u001b[38;5;66;03m# Fallback dispatch system (dispatch v1):\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1260\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch_target\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1261\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, \u001b[38;5;167;01mValueError\u001b[39;00m):\n\u001b[1;32m   1262\u001b[0m   \u001b[38;5;66;03m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[39;00m\n\u001b[1;32m   1263\u001b[0m   \u001b[38;5;66;03m# TypeError, when given unexpected types.  So we need to catch both.\u001b[39;00m\n\u001b[1;32m   1264\u001b[0m   result \u001b[38;5;241m=\u001b[39m dispatch(op_dispatch_handler, args, kwargs)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:161\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m\u001b[38;5;241m.\u001b[39mtf_export(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconvert_to_tensor\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m     97\u001b[0m \u001b[38;5;129m@dispatch\u001b[39m\u001b[38;5;241m.\u001b[39madd_dispatch_support\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_to_tensor_v2_with_dispatch\u001b[39m(\n\u001b[1;32m     99\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype_hint\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    100\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m tensor_lib\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[1;32m    101\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m  This function converts Python objects of various types to `Tensor`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;124;03m    ValueError: If the `value` is a tensor not of given `dtype` in graph mode.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 161\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconvert_to_tensor_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    162\u001b[0m \u001b[43m      \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype_hint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion.py:171\u001b[0m, in \u001b[0;36mconvert_to_tensor_v2\u001b[0;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# preferred_dtype = preferred_dtype or dtype_hint\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor_conversion_registry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreferred_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_hint\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/tensor_conversion_registry.py:234\u001b[0m, in \u001b[0;36mconvert\u001b[0;34m(value, dtype, name, as_ref, preferred_dtype, accepted_result_types)\u001b[0m\n\u001b[1;32m    225\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    226\u001b[0m           _add_error_prefix(\n\u001b[1;32m    227\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConversion function \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconversion_func\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m for type \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    230\u001b[0m               \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mactual = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mret\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mbase_dtype\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    231\u001b[0m               name\u001b[38;5;241m=\u001b[39mname))\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 234\u001b[0m   ret \u001b[38;5;241m=\u001b[39m \u001b[43mconversion_func\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mas_ref\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mas_ref\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m:\n\u001b[1;32m    237\u001b[0m   \u001b[38;5;28;01mcontinue\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_tensor_conversion.py:29\u001b[0m, in \u001b[0;36m_constant_tensor_conversion_function\u001b[0;34m(v, dtype, name, as_ref)\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mframework\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m constant_op  \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top\u001b[39;00m\n\u001b[1;32m     28\u001b[0m _ \u001b[38;5;241m=\u001b[39m as_ref\n\u001b[0;32m---> 29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mconstant_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconstant\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/ops/weak_tensor_ops.py:142\u001b[0m, in \u001b[0;36mweak_tensor_binary_op_wrapper.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    141\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mis_auto_dtype_conversion_enabled():\n\u001b[0;32m--> 142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m   bound_arguments \u001b[38;5;241m=\u001b[39m signature\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    144\u001b[0m   bound_arguments\u001b[38;5;241m.\u001b[39mapply_defaults()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:276\u001b[0m, in \u001b[0;36mconstant\u001b[0;34m(value, dtype, shape, name)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;129m@tf_export\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconstant\u001b[39m\u001b[38;5;124m\"\u001b[39m, v1\u001b[38;5;241m=\u001b[39m[])\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstant\u001b[39m(\n\u001b[1;32m    179\u001b[0m     value, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mConst\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    180\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Union[ops\u001b[38;5;241m.\u001b[39mOperation, ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase]:\n\u001b[1;32m    181\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant tensor from a tensor-like object.\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \n\u001b[1;32m    183\u001b[0m \u001b[38;5;124;03m  Note: All eager `tf.Tensor` values are immutable (in contrast to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;124;03m    ValueError: if called on a symbolic tensor.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m--> 276\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mallow_broadcast\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:289\u001b[0m, in \u001b[0;36m_constant_impl\u001b[0;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[1;32m    287\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf.constant\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    288\u001b[0m       \u001b[38;5;28;01mreturn\u001b[39;00m _constant_eager_impl(ctx, value, dtype, shape, verify_shape)\n\u001b[0;32m--> 289\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_constant_eager_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverify_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    291\u001b[0m const_tensor \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39m_create_graph_constant(  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     value, dtype, shape, name, verify_shape, allow_broadcast\n\u001b[1;32m    293\u001b[0m )\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m const_tensor\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:301\u001b[0m, in \u001b[0;36m_constant_eager_impl\u001b[0;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_constant_eager_impl\u001b[39m(\n\u001b[1;32m    298\u001b[0m     ctx, value, dtype, shape, verify_shape\n\u001b[1;32m    299\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ops\u001b[38;5;241m.\u001b[39m_EagerTensorBase:\n\u001b[1;32m    300\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Creates a constant on the current device.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 301\u001b[0m   t \u001b[38;5;241m=\u001b[39m \u001b[43mconvert_to_eager_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m shape \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    303\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tensorflow/python/framework/constant_op.py:108\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m    106\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m    107\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m--> 108\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import time\n",
    "\n",
    "total_loss_list = []\n",
    "train_accuracy_list = []\n",
    "val_loss_list = []\n",
    "val_accuracy_list = []\n",
    "\n",
    "# Define CORAL loss function\n",
    "def coral_loss(source, target):\n",
    "    d = tf.cast(tf.shape(source)[1], tf.float32)\n",
    "    \n",
    "    # Centering the data\n",
    "    source_centered = source - tf.reduce_mean(source, axis=0)\n",
    "    target_centered = target - tf.reduce_mean(target, axis=0)\n",
    "    \n",
    "    # Covariance matrices\n",
    "    source_cov = tf.matmul(tf.transpose(source_centered), source_centered) / (tf.cast(tf.shape(source)[0], tf.float32) - 1)\n",
    "    target_cov = tf.matmul(tf.transpose(target_centered), target_centered) / (tf.cast(tf.shape(target)[0], tf.float32) - 1)\n",
    "    \n",
    "    # CORAL loss is the Frobenius norm of the difference between covariance matrices\n",
    "    loss = tf.reduce_sum(tf.square(source_cov - target_cov)) / (4 * d * d)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Custom ResNet-18 model with added CORAL Loss function\n",
    "\n",
    "The model outputs both feature representations and class predictions, making this suitable for seamless CORAL Loss integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom ResNet18 model that includes the additional CORAL loss\n",
    "def create_coral_resnet18_model(input_shape=(224, 224, 3), num_classes=10):\n",
    "    print(\"Creating ResNet18-like model from ResNet50...\")\n",
    "    base_model = tf.keras.applications.ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)\n",
    "    \n",
    "    # Feature extractor layer (Global Average Pooling)\n",
    "    feature_extractor = GlobalAveragePooling2D()(base_model.output)\n",
    "    \n",
    "    # Classification head\n",
    "    classifier = Dense(512, activation='relu')(feature_extractor)\n",
    "    predictions = Dense(num_classes, activation='softmax')(classifier)\n",
    "    \n",
    "    # Create the model\n",
    "    model = Model(inputs=base_model.input, outputs=[feature_extractor, predictions])\n",
    "    \n",
    "    # Freeze base layers\n",
    "    print(\"Freezing layers...\")\n",
    "    for layer in base_model.layers[:-100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Custom Training Loop with CORAL Loss\n",
    "\n",
    "This code defines a **custom training loop** for training a model with the **CORAL loss** (Correlation Alignment Loss). The training loop includes both training and validation phases, where the model is trained on both source and target domain data, while aligning feature representations between them via CORAL loss. Here's a breakdown of the code:\n",
    "\n",
    "## 1. Function Definition\n",
    "```python\n",
    "def train_coral_model(source_data, target_data, model, val_data, epochs=10):\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inputs\n",
    "\n",
    "### `source_data`\n",
    "- **Type**: Tuple of (images, labels)\n",
    "- **Description**: The training data from the source domain. This includes images and corresponding labels. It is used for training the model to learn from a known dataset.\n",
    "  \n",
    "### `target_data`\n",
    "- **Type**: Tuple of (images)\n",
    "- **Description**: The training data from the target domain. This includes only images and no labels. The goal is to align the features from the target domain with those of the source domain using the CORAL loss.\n",
    "\n",
    "### `model`\n",
    "- **Type**: TensorFlow/Keras model\n",
    "- **Description**: The neural network model to be trained. This model can be a deep learning architecture such as ResNet or a custom-built model, which will use both source and target domain data during training.\n",
    "\n",
    "### `val_data`\n",
    "- **Type**: Tuple of (images, labels)\n",
    "- **Description**: The validation data to monitor the model's performance after each epoch. It contains images and corresponding labels, used to calculate validation loss and accuracy.\n",
    "\n",
    "### `epochs`\n",
    "- **Type**: Integer\n",
    "- **Default**: 10\n",
    "- **Description**: The number of epochs to train the model. Each epoch corresponds to one full pass through the source and target training datasets. The more epochs, the more the model is trained, but it may also increase the risk of overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For Each Batch:\n",
    "\n",
    "## Forward Pass:\n",
    "The model computes feature and predictions on the source domain and target domain\n",
    "\n",
    "#### Loss Calculation:\n",
    "#### The model calculates 2 losses:\n",
    "\n",
    "+ Categorical Cross Entropy between true and predicted labels\n",
    "\n",
    "+ CORAL Loss, which is used for backpropagation. The CORAL Loss uses the covariance matrices extracted from the intermediate layers of the ResNet-18\n",
    "\n",
    "+ Total loss, summing up Categorical Cross Entropy and CORAL\n",
    "\n",
    "\n",
    "\n",
    "Gradients of the total loss with respect to the model's tunable variables are backpropagated using ```GradientTape```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom training loop to incorporate CORAL loss\n",
    "def train_coral_model(source_data, target_data, model, val_data, epochs=10):\n",
    "    optimizer = Adam()\n",
    "    train_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    val_accuracy_metric = tf.keras.metrics.CategoricalAccuracy()\n",
    "    \n",
    "    # Calculate the number of iterations based on the shortest dataset\n",
    "    iterations_per_epoch = min(len(source_data), len(target_data))\n",
    "    print(f'Iterations per epoch: {iterations_per_epoch}')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        print(f'\\nStarting Epoch {epoch+1}/{epochs}...')\n",
    "        epoch_losses = []\n",
    "        epoch_accuracies = []\n",
    "        \n",
    "        # Training phase\n",
    "        start_time = time.time()\n",
    "        for batch_num, ((source_images, source_labels), target_images) in enumerate(zip(source_data, target_data), start=1):\n",
    "            if batch_num > iterations_per_epoch:\n",
    "                break  # Ensure only the correct number of batches are processed\n",
    "            \n",
    "            print(f'Processing Batch {batch_num}/{iterations_per_epoch}...')\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            with tf.GradientTape() as tape:\n",
    "                # Forward pass for source domain\n",
    "                source_features, source_preds = model(source_images, training=True)\n",
    "                print(f'Source forward pass complete for Batch {batch_num}')\n",
    "                \n",
    "                # Forward pass for target domain\n",
    "                target_features, _ = model(target_images, training=True)\n",
    "                print(f'Target forward pass complete for Batch {batch_num}')\n",
    "                \n",
    "                # Compute cross-entropy loss on source domain\n",
    "                classification_loss = tf.keras.losses.categorical_crossentropy(source_labels, source_preds)\n",
    "                \n",
    "                # Compute CORAL loss to align features\n",
    "                coral_loss_value = coral_loss(source_features, target_features)\n",
    "                \n",
    "                # Total loss\n",
    "                total_loss = tf.reduce_mean(classification_loss) + coral_loss_value\n",
    "                \n",
    "                # Backpropagation\n",
    "                gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "                optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "                \n",
    "                # Track loss\n",
    "                epoch_losses.append(total_loss.numpy())\n",
    "                \n",
    "                # Update accuracy metric\n",
    "                train_accuracy_metric.update_state(source_labels, source_preds)\n",
    "            \n",
    "            batch_end_time = time.time()\n",
    "            print(f'Batch {batch_num} processed in {batch_end_time - batch_start_time:.2f} seconds')\n",
    "        \n",
    "        # Calculate mean accuracy for the epoch\n",
    "        epoch_accuracy = train_accuracy_metric.result().numpy()\n",
    "        train_accuracy_list.append(epoch_accuracy)\n",
    "        train_accuracy_metric.reset_state()\n",
    "        \n",
    "        # Calculate mean loss for the epoch\n",
    "        epoch_loss = np.mean(epoch_losses)\n",
    "        total_loss_list.append(epoch_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "        print('Starting validation phase...')\n",
    "        val_start_time = time.time()\n",
    "        for val_batch_num, (val_images, val_labels) in enumerate(val_data, start=1):\n",
    "            val_features, val_preds = model(val_images, training=False)\n",
    "            val_classification_loss = tf.keras.losses.categorical_crossentropy(val_labels, val_preds)\n",
    "            val_loss = tf.reduce_mean(val_classification_loss).numpy()\n",
    "            val_losses.append(val_loss)\n",
    "            \n",
    "            # Update accuracy metric\n",
    "            val_accuracy_metric.update_state(val_labels, val_preds)\n",
    "        \n",
    "        # Calculate mean validation loss and accuracy\n",
    "        val_accuracy = val_accuracy_metric.result().numpy()\n",
    "        val_loss_mean = np.mean(val_losses)\n",
    "        val_loss_list.append(val_loss_mean)\n",
    "        val_accuracy_list.append(val_accuracy)\n",
    "        val_accuracy_metric.reset_state()\n",
    "        \n",
    "        val_end_time = time.time()\n",
    "        \n",
    "        # Print epoch results\n",
    "        epoch_duration = time.time() - start_time\n",
    "        print(f'Epoch {epoch+1} - Training Loss: {epoch_loss} - Training Accuracy: {epoch_accuracy}')\n",
    "        print(f'Validation Loss: {val_loss_mean} - Validation Accuracy: {val_accuracy}')\n",
    "        print(f'Epoch Duration: {epoch_duration:.2f} seconds (Validation took {val_end_time - val_start_time:.2f} seconds)')\n",
    "    \n",
    "    return total_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load the data using ImageDataGenerator for both DSLR (source) and Webcam (target)\n",
    "source_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "target_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Load images from DSLR domain (source domain) with only the first 10 classes\n",
    "print('Loading source training data...')\n",
    "source_train_gen = source_datagen.flow_from_directory(\n",
    "    'Office-31/dslr',  # DSLR domain directory\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    classes=['back_pack','bike','bike_helmet','bookcase','bottle','calculator','desk_chair','desk_lamp','desktop_computer','file_cabinet']\n",
    ")\n",
    "\n",
    "# Load images from Webcam domain (target domain) with only the first 10 classes\n",
    "print('Loading target training data...')\n",
    "target_train_gen = target_datagen.flow_from_directory(\n",
    "    'Office-31/webcam',  # Webcam domain directory\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode=None,\n",
    "    classes=['back_pack','bike','bike_helmet','bookcase','bottle','calculator','desk_chair','desk_lamp','desktop_computer','file_cabinet']\n",
    ")\n",
    "\n",
    "# Load validation data for source domain with only the first 10 classes\n",
    "print('Loading validation data...')\n",
    "source_val_gen = source_datagen.flow_from_directory(\n",
    "    'Office-31/dslr',  # Adjust to your source validation data\n",
    "    target_size=(224, 224),\n",
    "    batch_size=64,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    classes=['back_pack','bike','bike_helmet','bookcase','bottle','calculator','desk_chair','desk_lamp','desktop_computer','file_cabinet']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CORAL ResNet-18 is trained on DSLR, and target is Webcam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/GPU:0\"):\n",
    "    # Instantiate and train the CORAL ResNet18 model\n",
    "    coral_model = create_coral_resnet18_model(num_classes=10)\n",
    "    print(\"Starting training...\")\n",
    "    total_loss_list, train_accuracy_list, val_loss_list, val_accuracy_list = train_coral_model(\n",
    "        source_train_gen, target_train_gen, coral_model, source_val_gen, epochs=10\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mevaluate(webcam_generator)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "import sklearn.metrics\n",
    "model.evaluate(webcam_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluate the divergence of the Webcam and DSLR through the Kolmogorov-Smirnov Test and Jensen-Shannon Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Webcam and DSLR features are extracted\n",
    "\n",
    "+ The ResNet18 model is loaded with pre-trained weights from ImageNet.\n",
    "\n",
    "+ The final classification layer is removed to keep only the convolutional layers for feature extraction.\n",
    "\n",
    "+ The model is set to evaluation mode using eval().\n",
    "\n",
    "+ Image transformations are defined (resize to 224x224, convert to tensor, and normalize using ImageNet statistics).\n",
    "\n",
    "+ The Office-31 dataset is loaded from the \"webcam\" and \"dslr\" folders using ImageFolder and transformations are applied.\n",
    "\n",
    "+ DataLoader objects are created for batch processing of the datasets.\n",
    "\n",
    "+ A function ```extract_features``` is defined to pass the images through the model and extract features, which are flattened and concatenated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /Users/adityachakraborty/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:00<00:00, 51.2MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Webcam features shape: (235, 512)\n",
      "DSLR features shape: (154, 512)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, datasets, transforms\n",
    "\n",
    "# Load pre-trained ResNet18\n",
    "resnet18 = models.resnet18(pretrained=True)\n",
    "\n",
    "# Remove the final classification layer to get features\n",
    "feature_extractor = nn.Sequential(*list(resnet18.children())[:-1])\n",
    "feature_extractor.eval()\n",
    "\n",
    "# Define transformations (resize to 224x224 and normalize)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Load Office-31 dataset (replace with the actual path to your dataset)\n",
    "webcam_dataset = datasets.ImageFolder(root=\"Office-31/webcam\", transform=transform)\n",
    "dslr_dataset = datasets.ImageFolder(root=\"Office-31/dslr\", transform=transform)\n",
    "\n",
    "# Create DataLoaders for batch processing\n",
    "webcam_loader = torch.utils.data.DataLoader(webcam_dataset, batch_size=32, shuffle=False)\n",
    "dslr_loader = torch.utils.data.DataLoader(dslr_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Function to extract features\n",
    "def extract_features(loader, model):\n",
    "    features = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, _ in loader:\n",
    "            output = model(inputs)\n",
    "            features.append(output.view(output.size(0), -1))  # Flatten the features\n",
    "    return torch.cat(features)\n",
    "\n",
    "# Extract features for webcam and DSLR domains\n",
    "webcam_features = extract_features(webcam_loader, feature_extractor).numpy()\n",
    "dslr_features = extract_features(dslr_loader, feature_extractor).numpy()\n",
    "\n",
    "print(f\"Webcam features shape: {webcam_features.shape}\")\n",
    "print(f\"DSLR features shape: {dslr_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolmogorov-Smirnov Test p-value: 0.2750561766887005\n",
      "Jensen-Shannon Divergence: 0.2684382783804823\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "# Kolmogorov-Smirnov (KS) Test\n",
    "def ks_test(webcam, dslr):\n",
    "    p_values = []\n",
    "    for i in range(webcam.shape[1]):\n",
    "        # Perform KS test for each feature (column)\n",
    "        ks_stat, p_value = stats.ks_2samp(webcam[:, i], dslr[:, i])\n",
    "        p_values.append(p_value)\n",
    "    return np.mean(p_values)  # Average p-value over all feature dimensions\n",
    "\n",
    "ks_p_value = ks_test(webcam_features, dslr_features)\n",
    "print(f\"Kolmogorov-Smirnov Test p-value: {ks_p_value}\")\n",
    "\n",
    "# Jensen-Shannon Divergence\n",
    "def js_divergence(webcam, dslr):\n",
    "    js_values = []\n",
    "    for i in range(webcam.shape[1]):\n",
    "        # Calculate JS divergence for each feature (column)\n",
    "        webcam_dist = np.histogram(webcam[:, i], bins=50, density=True)[0]\n",
    "        dslr_dist = np.histogram(dslr[:, i], bins=50, density=True)[0]\n",
    "        \n",
    "        # Normalize the distributions to avoid NaN values\n",
    "        webcam_dist /= webcam_dist.sum()\n",
    "        dslr_dist /= dslr_dist.sum()\n",
    "        \n",
    "        # Compute the Jensen-Shannon divergence\n",
    "        js_val = jensenshannon(webcam_dist, dslr_dist)\n",
    "        js_values.append(js_val)\n",
    "    return np.mean(js_values)  # Average JS divergence over all feature dimensions\n",
    "\n",
    "js_div = js_divergence(webcam_features, dslr_features)\n",
    "print(f\"Jensen-Shannon Divergence: {js_div}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
